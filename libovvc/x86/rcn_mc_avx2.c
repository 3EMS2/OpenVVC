#include "ovmem.h"
#include "rcn_structures.h"
#include <immintrin.h>
#include <stdint.h>
#include <string.h>

#define SIZE_BLOCK_2 0
#define SIZE_BLOCK_4 1
#define SIZE_BLOCK_8 2
#define SIZE_BLOCK_16 3
#define SIZE_BLOCK_32 4
#define SIZE_BLOCK_64 5
#define SIZE_BLOCK_128 6

#define MAX_PB_SIZE 128

#define EPEL_EXTRA_BEFORE 1
#define EPEL_EXTRA_AFTER 2
#define EPEL_EXTRA EPEL_EXTRA_BEFORE + EPEL_EXTRA_AFTER

#define QPEL_EXTRA_BEFORE 3
#define QPEL_EXTRA_AFTER 4
#define QPEL_EXTRA QPEL_EXTRA_BEFORE + QPEL_EXTRA_AFTER

#define BIT_DEPTH 10

#define WEIGHTED 1
#if WEIGHTED



DECLARE_ALIGNED(16, static const int16_t, ov_mc_filters_rpr_avx2[6][16][8]) =
{
    {{  0, 0,   0, 64,  0,  0,  0,  0 },
    {   0, 1,  -3, 63,  4,  -2,  1,  0 },
    {  -1, 2,  -5, 62,  8,  -3,  1,  0 },
    {  -1, 3,  -8, 60, 13,  -4,  1,  0 },
    {  -1, 4, -10, 58, 17,  -5,  1,  0 },
    {  -1, 4, -11, 52, 26,  -8,  3, -1 },
    {  -1, 3,  -9, 47, 31, -10,  4, -1 },
    {  -1, 4, -11, 45, 34, -10,  4, -1 },
    {  -1, 4, -11, 40, 40, -11,  4, -1 },
    {  -1, 4, -10, 34, 45, -11,  4, -1 },
    {  -1, 4, -10, 31, 47,  -9,  3, -1 },
    {  -1, 3,  -8, 26, 52, -11,  4, -1 },
    {   0, 1,  -5, 17, 58, -10,  4, -1 },
    {   0, 1,  -4, 13, 60,  -8,  3, -1 },
    {   0, 1,  -3,  8, 62,  -5,  2, -1 },
    {   0, 1,  -2,  4, 63,  -3,  1,  0 }},

    // 1.5x
    {{ -1, -5, 17, 42, 17, -5, -1,  0 },
    {  0, -5, 15, 41, 19, -5, -1,  0 },
    {  0, -5, 13, 40, 21, -4, -1,  0 },
    {  0, -5, 11, 39, 24, -4, -2,  1 },
    {  0, -5,  9, 38, 26, -3, -2,  1 },
    {  0, -5,  7, 38, 28, -2, -3,  1 },
    {  1, -5,  5, 36, 30, -1, -3,  1 },
    {  1, -4,  3, 35, 32,  0, -4,  1 },
    {  1, -4,  2, 33, 33,  2, -4,  1 },
    {  1, -4,  0, 32, 35,  3, -4,  1 },
    {  1, -3, -1, 30, 36,  5, -5,  1 },
    {  1, -3, -2, 28, 38,  7, -5,  0 },
    {  1, -2, -3, 26, 38,  9, -5,  0 },
    {  1, -2, -4, 24, 39, 11, -5,  0 },
    {  0, -1, -4, 21, 40, 13, -5,  0 },
    {  0, -1, -5, 19, 41, 15, -5,  0 }},

    // 2.0x
    {{ -4,  2, 20, 28, 20,  2, -4,  0 },
    { -4,  0, 19, 29, 21,  5, -4, -2 },
    { -4, -1, 18, 29, 22,  6, -4, -2 },
    { -4, -1, 16, 29, 23,  7, -4, -2 },
    { -4, -1, 16, 28, 24,  7, -4, -2 },
    { -4, -1, 14, 28, 25,  8, -4, -2 },
    { -3, -3, 14, 27, 26,  9, -3, -3 },
    { -3, -1, 12, 28, 25, 10, -4, -3 },
    { -3, -3, 11, 27, 27, 11, -3, -3 },
    { -3, -4, 10, 25, 28, 12, -1, -3 },
    { -3, -3,  9, 26, 27, 14, -3, -3 },
    { -2, -4,  8, 25, 28, 14, -1, -4 },
    { -2, -4,  7, 24, 28, 16, -1, -4 },
    { -2, -4,  7, 23, 29, 16, -1, -4 },
    { -2, -4,  6, 22, 29, 18, -1, -4 },
    { -2, -4,  5, 21, 29, 19,  0, -4 }},

    // Affine
    {{  0, 0,  0, 64,  0,   0,  0,  0 },
    {  0, 1,  -3, 63,  4,  -2,  1,  0 },
    {  0, 1,  -5, 62,  8,  -3,  1,  0 },
    {  0, 2,  -8, 60, 13,  -4,  1,  0 },
    {  0, 3, -10, 58, 17,  -5,  1,  0 }, //1/4
    {  0, 3, -11, 52, 26,  -8,  2,  0 },
    {  0, 2,  -9, 47, 31, -10,  3,  0 },
    {  0, 3, -11, 45, 34, -10,  3,  0 },
    {  0, 3, -11, 40, 40, -11,  3,  0 }, //1/2
    {  0, 3, -10, 34, 45, -11,  3,  0 },
    {  0, 3, -10, 31, 47,  -9,  2,  0 },
    {  0, 2,  -8, 26, 52, -11,  3,  0 },
    {  0, 1,  -5, 17, 58, -10,  3,  0 }, //3/4
    {  0, 1,  -4, 13, 60,  -8,  2,  0 },
    {  0, 1,  -3,  8, 62,  -5,  1,  0 },
    {  0, 1,  -2,  4, 63,  -3,  1,  0 }},
    // 1.5x
    {{  0, -6, 17, 42, 17, -5, -1,  0 },
      {  0, -5, 15, 41, 19, -5, -1,  0 },
      {  0, -5, 13, 40, 21, -4, -1,  0 },
      {  0, -5, 11, 39, 24, -4, -1,  0 },
      {  0, -5,  9, 38, 26, -3, -1,  0 },
      {  0, -5,  7, 38, 28, -2, -2,  0 },
      {  0, -4,  5, 36, 30, -1, -2,  0 },
      {  0, -3,  3, 35, 32,  0, -3,  0 },
      {  0, -3,  2, 33, 33,  2, -3,  0 },
      {  0, -3,  0, 32, 35,  3, -3,  0 },
      {  0, -2, -1, 30, 36,  5, -4,  0 },
      {  0, -2, -2, 28, 38,  7, -5,  0 },
      {  0, -1, -3, 26, 38,  9, -5,  0 },
      {  0, -1, -4, 24, 39, 11, -5,  0 },
      {  0, -1, -4, 21, 40, 13, -5,  0 },
      {  0, -1, -5, 19, 41, 15, -5,  0 }},

    // 2x
    {{  0, -2, 20, 28, 20,  2, -4,  0 },
      {  0, -4, 19, 29, 21,  5, -6,  0 },
      {  0, -5, 18, 29, 22,  6, -6,  0 },
      {  0, -5, 16, 29, 23,  7, -6,  0 },
      {  0, -5, 16, 28, 24,  7, -6,  0 },
      {  0, -5, 14, 28, 25,  8, -6,  0 },
      {  0, -6, 14, 27, 26,  9, -6,  0 },
      {  0, -4, 12, 28, 25, 10, -7,  0 },
      {  0, -6, 11, 27, 27, 11, -6,  0 },
      {  0, -7, 10, 25, 28, 12, -4,  0 },
      {  0, -6,  9, 26, 27, 14, -6,  0 },
      {  0, -6,  8, 25, 28, 14, -5,  0 },
      {  0, -6,  7, 24, 28, 16, -5,  0 },
      {  0, -6,  7, 23, 29, 16, -5,  0 },
      {  0, -6,  6, 22, 29, 18, -5,  0 },
      {  0, -6,  5, 21, 29, 19, -4,  0 }}
};

DECLARE_ALIGNED(16, static const int16_t, ov_mc_filters_rpr_avx2_c[3][32][4]) =
{
    {
        {  0, 64,  0,  0 },
        { -1, 63,  2,  0 },
        { -2, 62,  4,  0 },
        { -2, 60,  7, -1 },
        { -2, 58, 10, -2 },
        { -3, 57, 12, -2 },
        { -4, 56, 14, -2 },
        { -4, 55, 15, -2 },
        { -4, 54, 16, -2 },
        { -5, 53, 18, -2 },
        { -6, 52, 20, -2 },
        { -6, 49, 24, -3 },
        { -6, 46, 28, -4 },
        { -5, 44, 29, -4 },
        { -4, 42, 30, -4 },
        { -4, 39, 33, -4 },
        { -4, 36, 36, -4 },
        { -4, 33, 39, -4 },
        { -4, 30, 42, -4 },
        { -4, 29, 44, -5 },
        { -4, 28, 46, -6 },
        { -3, 24, 49, -6 },
        { -2, 20, 52, -6 },
        { -2, 18, 53, -5 },
        { -2, 16, 54, -4 },
        { -2, 15, 55, -4 },
        { -2, 14, 56, -4 },
        { -2, 12, 57, -3 },
        { -2, 10, 58, -2 },
        { -1,  7, 60, -2 },
        {  0,  4, 62, -2 },
        {  0,  2, 63, -1 }
    },
    {
        { 12, 40, 12,  0 },
        { 11, 40, 13,  0 },
        { 10, 40, 15, -1 },
        {  9, 40, 16, -1 },
        {  8, 40, 17, -1 },
        {  8, 39, 18, -1 },
        {  7, 39, 19, -1 },
        {  6, 38, 21, -1 },
        {  5, 38, 22, -1 },
        {  4, 38, 23, -1 },
        {  4, 37, 24, -1 },
        {  3, 36, 25,  0 },
        {  3, 35, 26,  0 },
        {  2, 34, 28,  0 },
        {  2, 33, 29,  0 },
        {  1, 33, 30,  0 },
        {  1, 31, 31,  1 },
        {  0, 30, 33,  1 },
        {  0, 29, 33,  2 },
        {  0, 28, 34,  2 },
        {  0, 26, 35,  3 },
        {  0, 25, 36,  3 },
        { -1, 24, 37,  4 },
        { -1, 23, 38,  4 },
        { -1, 22, 38,  5 },
        { -1, 21, 38,  6 },
        { -1, 19, 39,  7 },
        { -1, 18, 39,  8 },
        { -1, 17, 40,  8 },
        { -1, 16, 40,  9 },
        { -1, 15, 40, 10 },
        {  0, 13, 40, 11 }
    },

    {
        { 17, 30, 17,  0 },
        { 17, 30, 18, -1 },
        { 16, 30, 18,  0 },
        { 16, 30, 18,  0 },
        { 15, 30, 18,  1 },
        { 14, 30, 18,  2 },
        { 13, 29, 19,  3 },
        { 13, 29, 19,  3 },
        { 12, 29, 20,  3 },
        { 11, 28, 21,  4 },
        { 10, 28, 22,  4 },
        { 10, 27, 22,  5 },
        {  9, 27, 23,  5 },
        {  9, 26, 24,  5 },
        {  8, 26, 24,  6 },
        {  7, 26, 25,  6 },
        {  7, 25, 25,  7 },
        {  6, 25, 26,  7 },
        {  6, 24, 26,  8 },
        {  5, 24, 26,  9 },
        {  5, 23, 27,  9 },
        {  5, 22, 27, 10 },
        {  4, 22, 28, 10 },
        {  4, 21, 28, 11 },
        {  3, 20, 29, 12 },
        {  3, 19, 29, 13 },
        {  3, 19, 29, 13 },
        {  2, 18, 30, 14 },
        {  1, 18, 30, 15 },
        {  0, 18, 30, 16 },
        {  0, 18, 30, 16 },
        { -1, 18, 30, 17 }
    }
};

DECLARE_ALIGNED(16, const int16_t, oh_hevc_epel_filters_avx2[31][2][16]) = {
  //{  0, 64,  0,  0 },
  { { -1, 63, -1, 63, -1, 63, -1, 63, -1, 63, -1, 63, -1, 63, -1, 63 } , { 2, 0, 2, 0, 2, 0, 2, 0 , 2, 0, 2, 0, 2, 0, 2, 0 } },
  { { -2, 62, -2, 62, -2, 62, -2, 62, -2, 62, -2, 62, -2, 62, -2, 62 } , { 4, 0, 4, 0, 4, 0, 4, 0 , 4, 0, 4, 0, 4, 0, 4, 0 } },
  { { -2, 60, -2, 60, -2, 60, -2, 60, -2, 60, -2, 60, -2, 60, -2, 60 } , { 7, -1, 7, -1, 7, -1, 7, -1 , 7, -1, 7, -1, 7, -1, 7, -1 } },
  { { -2, 58, -2, 58, -2, 58, -2, 58, -2, 58, -2, 58, -2, 58, -2, 58 } , { 10, -2, 10, -2, 10, -2, 10, -2 , 10, -2, 10, -2, 10, -2, 10, -2 } },
  { { -3, 57, -3, 57, -3, 57, -3, 57, -3, 57, -3, 57, -3, 57, -3, 57 } , { 12, -2, 12, -2, 12, -2, 12, -2 , 12, -2, 12, -2, 12, -2, 12, -2 } },
  { { -4, 56, -4, 56, -4, 56, -4, 56, -4, 56, -4, 56, -4, 56, -4, 56 } , { 14, -2, 14, -2, 14, -2, 14, -2 , 14, -2, 14, -2, 14, -2, 14, -2 } },
  { { -4, 55, -4, 55, -4, 55, -4, 55, -4, 55, -4, 55, -4, 55, -4, 55 } , { 15, -2, 15, -2, 15, -2, 15, -2 , 15, -2, 15, -2, 15, -2, 15, -2 } },
  { { -4, 54, -4, 54, -4, 54, -4, 54, -4, 54, -4, 54, -4, 54, -4, 54 } , { 16, -2, 16, -2, 16, -2, 16, -2 , 16, -2, 16, -2, 16, -2, 16, -2 } },
  { { -5, 53, -5, 53, -5, 53, -5, 53, -5, 53, -5, 53, -5, 53, -5, 53 } , { 18, -2, 18, -2, 18, -2, 18, -2 , 18, -2, 18, -2, 18, -2, 18, -2 } },
  { { -6, 52, -6, 52, -6, 52, -6, 52, -6, 52, -6, 52, -6, 52, -6, 52 } , { 20, -2, 20, -2, 20, -2, 20, -2 , 20, -2, 20, -2, 20, -2, 20, -2 } },
  { { -6, 49, -6, 49, -6, 49, -6, 49, -6, 49, -6, 49, -6, 49, -6, 49 } , { 24, -3, 24, -3, 24, -3, 24, -3 , 24, -3, 24, -3, 24, -3, 24, -3 } },
  { { -6, 46, -6, 46, -6, 46, -6, 46, -6, 46, -6, 46, -6, 46, -6, 46 } , { 28, -4, 28, -4, 28, -4, 28, -4 , 28, -4, 28, -4, 28, -4, 28, -4 } },
  { { -5, 44, -5, 44, -5, 44, -5, 44, -5, 44, -5, 44, -5, 44, -5, 44 } , { 29, -4, 29, -4, 29, -4, 29, -4 , 29, -4, 29, -4, 29, -4, 29, -4 } },
  { { -4, 42, -4, 42, -4, 42, -4, 42, -4, 42, -4, 42, -4, 42, -4, 42 } , { 30, -4, 30, -4, 30, -4, 30, -4 , 30, -4, 30, -4, 30, -4, 30, -4 } },
  { { -4, 39, -4, 39, -4, 39, -4, 39, -4, 39, -4, 39, -4, 39, -4, 39 } , { 33, -4, 33, -4, 33, -4, 33, -4 , 33, -4, 33, -4, 33, -4, 33, -4 } },
  { { -4, 36, -4, 36, -4, 36, -4, 36, -4, 36, -4, 36, -4, 36, -4, 36 } , { 36, -4, 36, -4, 36, -4, 36, -4 , 36, -4, 36, -4, 36, -4, 36, -4 } },
  { { -4, 33, -4, 33, -4, 33, -4, 33, -4, 33, -4, 33, -4, 33, -4, 33 } , { 39, -4, 39, -4, 39, -4, 39, -4 , 39, -4, 39, -4, 39, -4, 39, -4 } },
  { { -4, 30, -4, 30, -4, 30, -4, 30, -4, 30, -4, 30, -4, 30, -4, 30 } , { 42, -4, 42, -4, 42, -4, 42, -4 , 42, -4, 42, -4, 42, -4, 42, -4 } },
  { { -4, 29, -4, 29, -4, 29, -4, 29, -4, 29, -4, 29, -4, 29, -4, 29 } , { 44, -5, 44, -5, 44, -5, 44, -5 , 44, -5, 44, -5, 44, -5, 44, -5 } },
  { { -4, 28, -4, 28, -4, 28, -4, 28, -4, 28, -4, 28, -4, 28, -4, 28 } , { 46, -6, 46, -6, 46, -6, 46, -6 , 46, -6, 46, -6, 46, -6, 46, -6 } },
  { { -3, 24, -3, 24, -3, 24, -3, 24, -3, 24, -3, 24, -3, 24, -3, 24 } , { 49, -6, 49, -6, 49, -6, 49, -6 , 49, -6, 49, -6, 49, -6, 49, -6 } },
  { { -2, 20, -2, 20, -2, 20, -2, 20, -2, 20, -2, 20, -2, 20, -2, 20 } , { 52, -6, 52, -6, 52, -6, 52, -6 , 52, -6, 52, -6, 52, -6, 52, -6 } },
  { { -2, 18, -2, 18, -2, 18, -2, 18, -2, 18, -2, 18, -2, 18, -2, 18 } , { 53, -5, 53, -5, 53, -5, 53, -5 , 53, -5, 53, -5, 53, -5, 53, -5 } },
  { { -2, 16, -2, 16, -2, 16, -2, 16, -2, 16, -2, 16, -2, 16, -2, 16 } , { 54, -4, 54, -4, 54, -4, 54, -4 , 54, -4, 54, -4, 54, -4, 54, -4 } },
  { { -2, 15, -2, 15, -2, 15, -2, 15, -2, 15, -2, 15, -2, 15, -2, 15 } , { 55, -4, 55, -4, 55, -4, 55, -4 , 55, -4, 55, -4, 55, -4, 55, -4 } },
  { { -2, 14, -2, 14, -2, 14, -2, 14, -2, 14, -2, 14, -2, 14, -2, 14 } , { 56, -4, 56, -4, 56, -4, 56, -4 , 56, -4, 56, -4, 56, -4, 56, -4 } },
  { { -2, 12, -2, 12, -2, 12, -2, 12, -2, 12, -2, 12, -2, 12, -2, 12 } , { 57, -3, 57, -3, 57, -3, 57, -3 , 57, -3, 57, -3, 57, -3, 57, -3 } },
  { { -2, 10, -2, 10, -2, 10, -2, 10, -2, 10, -2, 10, -2, 10, -2, 10 } , { 58, -2, 58, -2, 58, -2, 58, -2 , 58, -2, 58, -2, 58, -2, 58, -2 } },
  { { -1, 7, -1, 7, -1, 7, -1, 7 , -1, 7, -1, 7, -1, 7, -1, 7 } , { 60, -2, 60, -2, 60, -2, 60, -2 , 60, -2, 60, -2, 60, -2, 60, -2 } },
  { { 0, 4, 0, 4, 0, 4, 0, 4 , 0, 4, 0, 4, 0, 4, 0, 4 } , { 62, -2, 62, -2, 62, -2, 62, -2 , 62, -2, 62, -2, 62, -2, 62, -2 } },
  { { 0, 2, 0, 2, 0, 2, 0, 2 , 0, 2, 0, 2, 0, 2, 0, 2 } , { 63, -1, 63, -1, 63, -1, 63, -1 , 63, -1, 63, -1, 63, -1, 63, -1 } },
};

DECLARE_ALIGNED(16, const int16_t, ov_mc_filters_rpr_avx2_v_c[3][32][2][8]) =
{
  {
      { { 0, 64, 0, 64, 0, 64, 0, 64 }, { 0, 0, 0, 0, 0, 0, 0, 0 } },
      { { -1, 63, -1, 63, -1, 63, -1, 63 }, { 2, 0, 2, 0, 2, 0, 2, 0 } },
      { { -2, 62, -2, 62, -2, 62, -2, 62 }, { 4, 0, 4, 0, 4, 0, 4, 0 } },
      { { -2, 60, -2, 60, -2, 60, -2, 60 }, { 7, -1, 7, -1, 7, -1, 7, -1 } },
      { { -2, 58, -2, 58, -2, 58, -2, 58 }, { 10, -2, 10, -2, 10, -2, 10, -2 } },
      { { -3, 57, -3, 57, -3, 57, -3, 57 }, { 12, -2, 12, -2, 12, -2, 12, -2 } },
      { { -4, 56, -4, 56, -4, 56, -4, 56 }, { 14, -2, 14, -2, 14, -2, 14, -2 } },
      { { -4, 55, -4, 55, -4, 55, -4, 55 }, { 15, -2, 15, -2, 15, -2, 15, -2 } },
      { { -4, 54, -4, 54, -4, 54, -4, 54 }, { 16, -2, 16, -2, 16, -2, 16, -2 } },
      { { -5, 53, -5, 53, -5, 53, -5, 53 }, { 18, -2, 18, -2, 18, -2, 18, -2 } },
      { { -6, 52, -6, 52, -6, 52, -6, 52 }, { 20, -2, 20, -2, 20, -2, 20, -2 } },
      { { -6, 49, -6, 49, -6, 49, -6, 49 }, { 24, -3, 24, -3, 24, -3, 24, -3 } },
      { { -6, 46, -6, 46, -6, 46, -6, 46 }, { 28, -4, 28, -4, 28, -4, 28, -4 } },
      { { -5, 44, -5, 44, -5, 44, -5, 44 }, { 29, -4, 29, -4, 29, -4, 29, -4 } },
      { { -4, 42, -4, 42, -4, 42, -4, 42 }, { 30, -4, 30, -4, 30, -4, 30, -4 } },
      { { -4, 39, -4, 39, -4, 39, -4, 39 }, { 33, -4, 33, -4, 33, -4, 33, -4 } },
      { { -4, 36, -4, 36, -4, 36, -4, 36 }, { 36, -4, 36, -4, 36, -4, 36, -4 } },
      { { -4, 33, -4, 33, -4, 33, -4, 33 }, { 39, -4, 39, -4, 39, -4, 39, -4 } },
      { { -4, 30, -4, 30, -4, 30, -4, 30 }, { 42, -4, 42, -4, 42, -4, 42, -4 } },
      { { -4, 29, -4, 29, -4, 29, -4, 29 }, { 44, -5, 44, -5, 44, -5, 44, -5 } },
      { { -4, 28, -4, 28, -4, 28, -4, 28 }, { 46, -6, 46, -6, 46, -6, 46, -6 } },
      { { -3, 24, -3, 24, -3, 24, -3, 24 }, { 49, -6, 49, -6, 49, -6, 49, -6 } },
      { { -2, 20, -2, 20, -2, 20, -2, 20 }, { 52, -6, 52, -6, 52, -6, 52, -6 } },
      { { -2, 18, -2, 18, -2, 18, -2, 18 }, { 53, -5, 53, -5, 53, -5, 53, -5 } },
      { { -2, 16, -2, 16, -2, 16, -2, 16 }, { 54, -4, 54, -4, 54, -4, 54, -4 } },
      { { -2, 15, -2, 15, -2, 15, -2, 15 }, { 55, -4, 55, -4, 55, -4, 55, -4 } },
      { { -2, 14, -2, 14, -2, 14, -2, 14 }, { 56, -4, 56, -4, 56, -4, 56, -4 } },
      { { -2, 12, -2, 12, -2, 12, -2, 12 }, { 57, -3, 57, -3, 57, -3, 57, -3 } },
      { { -2, 10, -2, 10, -2, 10, -2, 10 }, { 58, -2, 58, -2, 58, -2, 58, -2 } },
      { { -1, 7, -1, 7, -1, 7, -1, 7 }, { 60, -2, 60, -2, 60, -2, 60, -2 } },
      { { 0, 4, 0, 4, 0, 4, 0, 4 }, { 62, -2, 62, -2, 62, -2, 62, -2 } },
      { { 0, 2, 0, 2, 0, 2, 0, 2 }, { 63, -1, 63, -1, 63, -1, 63, -1 } },
  },
    {
        {{ 12, 40, 12, 40, 12, 40, 12, 40},{12,  0, 12,  0, 12,  0, 12,  0 }},
        {{ 11, 40, 11, 40, 11, 40, 11, 40}, {13,  0, 13,  0, 13,  0, 13,  0 }},
        {{ 10, 40,  10, 40,  10, 40,  10, 40}, { 15, -1, 15, -1, 15, -1, 15, -1 }},
        {{  9, 40,  9, 40,  9, 40,  9, 40} , { 16, -1, 16, -1, 16, -1, 16, -1 }},
        {{  8, 40,  8, 40,  8, 40,  8, 40}, { 17, -1, 17, -1, 17, -1, 17, -1 }},
        {{  8, 39,  8, 39,  8, 39, 8, 39},  { 18, -1, 18, -1, 18, -1, 18, -1 }},
        {{  7, 39, 7, 39, 7, 39, 7, 39}, { 19, -1, 19, -1, 19, -1, 19, -1 }},
        {{  6, 38,  6, 38,  6, 38,  6, 38}, { 21, -1,  21, -1, 21, -1, 21, -1 }},
        {{  5, 38, 5, 38, 5, 38, 5, 38}, { 22, -1,  22, -1,  22, -1, 22, -1 }},
        {{  4, 38, 4, 38, 4, 38, 4, 38}, {  23, -1, 23, -1, 23, -1, 23, -1 }},
        {{  4, 37, 4, 37, 4, 37, 4, 37}, { 24, -1, 24, -1, 24, -1, 24, -1 }},
        {{  3, 36, 3, 36, 3, 36, 3, 36}, { 25,  0,  25,  0, 25,  0, 25,  0 }},
        {{  3, 35, 3, 35, 3, 35, 3, 35}, { 26,  0,  26,  0, 26,  0, 26,  0 }},
        {{  2, 34, 2, 34, 2, 34, 2, 34},{ 28,  0, 28,  0, 28,  0, 28,  0 }},
        {{  2, 33, 2, 33, 2, 33, 2, 33},{ 29,  0, 29,  0, 29,  0, 29,  0 }},
        {{  1, 33, 1, 33, 1, 33, 1, 33},{ 30,  0, 30,  0, 30,  0, 30,  0 }},
        {{  1, 31, 1, 31, 1, 31, 1, 31},{ 31,  1, 31,  1, 31,  1, 31,  1 }},
        {{  0, 30,  0, 30,  0, 30,  0, 30},{ 33,  1, 33,  1, 33,  1, 33,  1 }},
        {{  0, 29,  0, 29,  0, 29,  0, 29},{ 33,  2, 33,  2, 33,  2, 33,  2 }},
        {{  0, 28,  0, 28,  0, 28,  0, 28},{ 34,  2, 34,  2, 34,  2, 34,  2 }},
        {{  0, 26,  0, 26,  0, 26,  0, 26},{ 35,  3, 35,  3, 35,  3, 35,  3 }},
        {{  0, 25,  0, 25,  0, 25,  0, 25},{ 36,  3, 36,  3, 36,  3, 36,  3 }},
        {{ -1, 24,-1, 24,-1, 24,-1, 24},{ 37,  4, 37,  4, 37,  4, 37,  4 }},
        {{ -1, 23,-1, 23,-1, 23,-1, 23},{ 38,  4, 38,  4, 38,  4, 38,  4 }},
        {{ -1, 22,-1, 22,-1, 22,-1, 22},{ 38,  5, 38,  5, 38,  5, 38,  5 }},
        {{ -1, 21,-1, 21,-1, 21,-1, 21},{ 38,  6, 38,  6, 38,  6, 38,  6 }},
        {{ -1, 19,-1, 19,-1, 19,-1, 19},{ 39,  7, 39,  7, 39,  7, 39,  7 }},
        {{ -1, 18,-1, 18,-1, 18,-1, 18},{ 39,  8, 39,  8, 39,  8, 39,  8 }},
        {{ -1, 17,-1, 17,-1, 17,-1, 17},{ 40,  8, 40,  8, 40,  8, 40,  8 }},
        {{ -1, 16,-1, 16,-1, 16,-1, 16},{ 40,  9, 40,  9, 40,  9, 40,  9 }},
        {{ -1, 15,-1, 15,-1, 15,-1, 15},{ 40, 10, 40, 10, 40, 10, 40, 10 }},
        {{  0, 13,  0, 13,  0, 13,  0, 13},{ 40, 11, 40, 11, 40, 11, 40, 11 }}
    },

    {
        {{ 17, 30, 17, 30, 17, 30, 17, 30},{ 17,  0,  17,  0,  17,  0,  17,  0 }},
        {{ 17, 30, 17, 30, 17, 30, 17, 30},{ 18, -1,  18, -1,  18, -1,  18, -1 }},
        {{ 16, 30, 16, 30, 16, 30, 16, 30},{ 18,  0,  18,  0,  18,  0,  18,  0 }},
        {{ 16, 30, 16, 30, 16, 30, 16, 30},{ 18,  0,  18,  0,  18,  0,  18,  0 }},
        {{ 15, 30, 15, 30, 15, 30, 15, 30},{ 18,  1,  18,  1,  18,  1,  18,  1 }},
        {{ 14, 30, 14, 30, 14, 30, 14, 30},{ 18,  2,  18,  2,  18,  2,  18,  2 }},
        {{ 13, 29, 13, 29, 13, 29, 13, 29},{ 19,  3,  19,  3,  19,  3,  19,  3 }},
        {{ 13, 29, 13, 29, 13, 29, 13, 29},{ 19,  3,  19,  3,  19,  3,  19,  3 }},
        {{ 12, 29, 12, 29, 12, 29, 12, 29},{ 20,  3,  20,  3,  20,  3,  20,  3 }},
        {{ 11, 28, 11, 28, 11, 28, 11, 28},{ 21,  4,  21,  4,  21,  4,  21,  4 }},
        {{ 10, 28, 10, 28, 10, 28, 10, 28},{ 22,  4,  22,  4,  22,  4,  22,  4 }},
        {{ 10, 27, 10, 27, 10, 27, 10, 27},{ 22,  5,  22,  5,  22,  5,  22,  5 }},
        {{  9, 27,  9, 27,  9, 27,  9, 27},{ 23,  5,  23,  5,  23,  5,  23,  5 }},
        {{  9, 26,  9, 26,  9, 26,  9, 26},{ 24,  5,  24,  5,  24,  5,  24,  5 }},
        {{  8, 26,  8, 26,  8, 26,  8, 26},{ 24,  6,  24,  6,  24,  6,  24,  6 }},
        {{  7, 26, 7, 26, 7, 26, 7, 26},{ 25,  6,  25,  6,  25,  6,  25,  6 }},
        {{  7, 25, 7, 25, 7, 25, 7, 25},{ 25,  7,  25,  7,  25,  7,  25,  7 }},
        {{  6, 25, 6, 25, 6, 25, 6, 25},{ 26,  7,  26,  7,  26,  7,  26,  7 }},
        {{  6, 24, 6, 24, 6, 24, 6, 24},{ 26,  8,  26,  8,  26,  8,  26,  8 }},
        {{  5, 24, 5, 24, 5, 24, 5, 24},{ 26,  9,  26,  9,  26,  9,  26,  9 }},
        {{  5, 23, 5, 23, 5, 23, 5, 23},{ 27,  9,  27,  9,  27,  9,  27,  9 }},
        {{  5, 22, 5, 22, 5, 22, 5, 22},{ 27, 10,  27, 10,  27, 10,  27, 10 }},
        {{  4, 22, 4, 22, 4, 22, 4, 22},{ 28, 10,  28, 10,  28, 10,  28, 10 }},
        {{  4, 21, 4, 21, 4, 21, 4, 21},{ 28, 11,  28, 11,  28, 11,  28, 11 }},
        {{  3, 20, 3, 20, 3, 20, 3, 20},{ 29, 12,  29, 12,  29, 12,  29, 12 }},
        {{  3, 19, 3, 19, 3, 19, 3, 19},{ 29, 13,  29, 13,  29, 13,  29, 13 }},
        {{  3, 19, 3, 19, 3, 19, 3, 19},{ 29, 13,  29, 13,  29, 13,  29, 13 }},
        {{  2, 18, 2, 18, 2, 18, 2, 18},{ 30, 14,  30, 14,  30, 14,  30, 14 }},
        {{  1, 18, 1, 18, 1, 18, 1, 18},{ 30, 15,  30, 15,  30, 15,  30, 15 }},
        {{  0, 18, 0, 18, 0, 18, 0, 18},{ 30, 16,  30, 16,  30, 16,  30, 16 }},
        {{  0, 18,   0, 18,   0, 18,   0, 18},{ 30, 16,  30, 16,  30, 16,  30, 16 }},
        {{ -1, 18,  -1, 18,  -1, 18,  -1, 18},{ 30, 17,  30, 17,  30, 17,  30, 17 }}
    }
};

DECLARE_ALIGNED(16, const int16_t, ov_mc_filters_rpr_avx2_v[6][16][4][8]) =
{
    {{{  0, 0,  0, 0,  0, 0,  0, 0},{   0, 64,   0, 64,   0, 64,   0, 64},{  0,  0,  0,  0,   0,  0,  0,  0},{  0,  0,   0,  0, 0,  0, 0,  0  }},
    {{   0, 1,   0, 1,   0, 1,   0, 1},{  -3, 63,-3, 63,-3, 63,-3, 63},{  4,  -2,  4,  -2,   4,  -2,  4,  -2},{  1,  0,   1,  0, 1,  0, 1,  0  }},
    {{  -1, 2,  -1, 2,  -1, 2,  -1, 2},{  -5, 62,-5, 62,-5, 62,-5, 62},{  8,  -3,  8,  -3,   8,  -3,  8,  -3},{  1,  0,   1,  0, 1,  0, 1,  0  }},
    {{  -1, 3,  -1, 3,  -1, 3,  -1, 3},{  -8, 60,-8, 60,-8, 60,-8, 60},{ 13,  -4, 13,  -4,  13,  -4, 13,  -4},{  1,  0,   1,  0, 1,  0, 1,  0  }},
    {{  -1, 4,  -1, 4,  -1, 4,  -1, 4},{ -10, 58,-10, 58,-10, 58,-10, 58},{ 17,  -5, 17,  -5,  17,  -5, 17,  -5},{  1,  0,   1,  0, 1,  0, 1,  0  }},
    {{  -1, 4,  -1, 4,  -1, 4,  -1, 4},{ -11, 52,-11, 52,-11, 52,-11, 52},{ 26,  -8, 26,  -8,  26,  -8, 26,  -8},{  3, -1,   3, -1, 3, -1, 3, -1  }},
    {{  -1, 3,  -1, 3,  -1, 3,  -1, 3},{  -9, 47,-9, 47,-9, 47,-9, 47},{ 31, -10, 31, -10,  31, -10, 31, -10},{  4, -1,   4, -1, 4, -1, 4, -1  }},
    {{  -1, 4,  -1, 4,  -1, 4,  -1, 4},{ -11, 45,-11, 45,-11, 45,-11, 45},{ 34, -10, 34, -10,  34, -10, 34, -10},{  4, -1,   4, -1, 4, -1, 4, -1  }},
    {{  -1, 4,  -1, 4,  -1, 4,  -1, 4},{ -11, 40,-11, 40,-11, 40,-11, 40},{ 40, -11, 40, -11,  40, -11, 40, -11},{  4, -1,   4, -1, 4, -1, 4, -1  }},
    {{  -1, 4,  -1, 4,  -1, 4,  -1, 4},{ -10, 34,-10, 34,-10, 34,-10, 34},{ 45, -11, 45, -11,  45, -11, 45, -11},{  4, -1,   4, -1, 4, -1, 4, -1  }},
    {{  -1, 4,  -1, 4,  -1, 4,  -1, 4},{ -10, 31,-10, 31,-10, 31,-10, 31},{ 47,  -9, 47,  -9,  47,  -9, 47,  -9},{  3, -1,   3, -1, 3, -1, 3, -1  }},
    {{  -1, 3,  -1, 3,  -1, 3,  -1, 3},{  -8, 26,-8, 26,-8, 26,-8, 26},{ 52, -11, 52, -11,  52, -11, 52, -11},{  4, -1,   4, -1, 4, -1, 4, -1  }},
    {{   0, 1,   0, 1,   0, 1,   0, 1},{  -5, 17,-5, 17,-5, 17,-5, 17},{ 58, -10, 58, -10,  58, -10, 58, -10},{  4, -1,   4, -1, 4, -1, 4, -1  }},
    {{   0, 1,   0, 1,   0, 1,   0, 1},{  -4, 13,-4, 13,-4, 13,-4, 13},{ 60,  -8, 60,  -8,  60,  -8, 60,  -8},{  3, -1,   3, -1, 3, -1, 3, -1  }},
    {{   0, 1,   0, 1,   0, 1,   0, 1},{  -3,  8,-3,  8,-3,  8,-3,  8},{ 62,  -5, 62,  -5,  62,  -5, 62,  -5},{  2, -1,   2, -1, 2, -1, 2, -1  }},
    {{   0, 1,   0, 1,   0, 1,   0, 1},{  -2,  4,-2,  4,-2,  4,-2,  4},{ 63,  -3, 63,  -3,  63,  -3, 63,  -3},{  1,  0,   1,  0, 1,  0, 1,  0  }}},

    // 1.5x
    {
    {{ -1, -5, -1, -5, -1, -5, -1, -5},{ 17, 42, 17, 42, 17, 42, 17, 42}, { 17, -5,17, -5,17, -5,17, -5},{ -1,  0, -1,  0, -1,  0, -1,  0 }},
    {{  0, -5,  0, -5,  0, -5,  0, -5},{ 15, 41, 15, 41, 15, 41, 15, 41}, { 19, -5,19, -5,19, -5,19, -5},{ -1,  0, -1,  0, -1,  0, -1,  0 }},
    {{  0, -5,  0, -5,  0, -5,  0, -5},{ 13, 40, 13, 40, 13, 40, 13, 40}, { 21, -4,21, -4,21, -4,21, -4},{ -1,  0, -1,  0, -1,  0, -1,  0 }},
    {{  0, -5,  0, -5,  0, -5,  0, -5},{ 11, 39, 11, 39, 11, 39, 11, 39}, { 24, -4,24, -4,24, -4,24, -4},{ -2,  1, -2,  1, -2,  1, -2,  1 }},
    {{  0, -5,  0, -5,  0, -5,  0, -5},{  9, 38,  9, 38,  9, 38,  9, 38}, { 26, -3,26, -3,26, -3,26, -3},{ -2,  1, -2,  1, -2,  1, -2,  1 }},
    {{  0, -5,  0, -5,  0, -5,  0, -5},{  7, 38,  7, 38,  7, 38,  7, 38}, { 28, -2,28, -2,28, -2,28, -2},{ -3,  1, -3,  1, -3,  1, -3,  1 }},
    {{  1, -5,  1, -5,  1, -5,  1, -5},{  5, 36,  5, 36,  5, 36,  5, 36}, { 30, -1,30, -1,30, -1,30, -1},{ -3,  1, -3,  1, -3,  1, -3,  1 }},
    {{  1, -4,  1, -4,  1, -4,  1, -4},{  3, 35,  3, 35,  3, 35,  3, 35}, { 32,  0, 32,  0, 32,  0, 32,  0},{ -4,  1, -4,  1, -4,  1, -4,  1 }},
    {{  1, -4,  1, -4,  1, -4,  1, -4}, {  2, 33,  2, 33,  2, 33,  2, 33}, { 33,  2, 33,  2, 33,  2, 33,  2}, { -4,  1, -4,  1, -4,  1, -4,  1 }},
    {{  1, -4,  1, -4,  1, -4,  1, -4}, {  0, 32,  0, 32,  0, 32,  0, 32}, { 35,  3, 35,  3, 35,  3, 35,  3}, { -4,  1, -4,  1, -4,  1, -4,  1 }},
    {{  1, -3,  1, -3,  1, -3,  1, -3}, { -1, 30,-1, 30,-1, 30,-1, 30}, { 36,  5, 36,  5, 36,  5, 36,  5}, { -5,  1, -5,  1, -5,  1, -5,  1 }},
    {{  1, -3,  1, -3,  1, -3,  1, -3}, { -2, 28,-2, 28,-2, 28,-2, 28}, { 38,  7, 38,  7, 38,  7, 38,  7}, { -5,  0, -5,  0, -5,  0, -5,  0 }},
    {{  1, -2,  1, -2,  1, -2,  1, -2}, { -3, 26,-3, 26,-3, 26,-3, 26}, { 38,  9, 38,  9, 38,  9, 38,  9}, { -5,  0, -5,  0, -5,  0, -5,  0 }},
    {{  1, -2,  1, -2,  1, -2,  1, -2}, { -4, 24,-4, 24,-4, 24,-4, 24}, { 39, 11, 39, 11, 39, 11, 39, 11}, { -5,  0, -5,  0, -5,  0, -5,  0 }},
    {{  0, -1,  0, -1,  0, -1,  0, -1}, { -4, 21,-4, 21,-4, 21,-4, 21}, { 40, 13, 40, 13, 40, 13, 40, 13}, { -5,  0, -5,  0, -5,  0, -5,  0 }},
    {{  0, -1,  0, -1,  0, -1,  0, -1}, { -5, 19,-5, 19,-5, 19,-5, 19}, { 41, 15, 41, 15, 41, 15, 41, 15}, { -5,  0, -5,  0, -5,  0, -5,  0 }}},

    // 2.0x
    {
    {{ -4,  2, -4,  2, -4,  2, -4,  2}, { 20, 28, 20, 28, 20, 28, 20, 28}, { 20,  2, 20,  2, 20,  2, 20,  2}, { -4,  0,  -4,  0,  -4,  0,  -4,  0 }},
    {{ -4,  0, -4,  0, -4,  0, -4,  0}, { 19, 29, 19, 29, 19, 29, 19, 29}, { 21,  5, 21,  5, 21,  5, 21,  5}, { -4, -2,  -4, -2,  -4, -2,  -4, -2 }},
    {{ -4, -1, -4, -1, -4, -1, -4, -1}, { 18, 29, 18, 29, 18, 29, 18, 29}, { 22,  6, 22,  6, 22,  6, 22,  6}, { -4, -2,  -4, -2,  -4, -2,  -4, -2 }},
    {{ -4, -1, -4, -1, -4, -1, -4, -1}, { 16, 29, 16, 29, 16, 29, 16, 29}, { 23,  7, 23,  7, 23,  7, 23,  7}, { -4, -2,  -4, -2,  -4, -2,  -4, -2 }},
    {{ -4, -1,-4, -1,-4, -1,-4, -1}, { 16, 28, 16, 28, 16, 28, 16, 28}, { 24,  7, 24,  7, 24,  7, 24,  7}, { -4, -2,  -4, -2,  -4, -2,  -4, -2 }},
    {{ -4, -1, -4, -1, -4, -1, -4, -1}, { 14, 28, 14, 28, 14, 28, 14, 28}, { 25,  8, 25,  8, 25,  8, 25,  8}, { -4, -2,  -4, -2,  -4, -2,  -4, -2 }},
    {{ -3, -3, -3, -3, -3, -3, -3, -3}, { 14, 27, 14, 27, 14, 27, 14, 27}, { 26,  9, 26,  9, 26,  9, 26,  9}, { -3, -3,  -3, -3,  -3, -3,  -3, -3 }},
    {{ -3, -1, -3, -1, -3, -1, -3, -1}, { 12, 28, 12, 28, 12, 28, 12, 28}, { 25, 10, 25, 10, 25, 10, 25, 10}, { -4, -3,  -4, -3,  -4, -3,  -4, -3 }},
    {{ -3, -3, -3, -3, -3, -3, -3, -3}, { 11, 27, 11, 27, 11, 27, 11, 27}, { 27, 11, 27, 11, 27, 11, 27, 11}, {-3, -3, -3, -3, -3, -3, -3, -3} },
    {{ -3, -4, -3, -4, -3, -4, -3, -4}, { 10, 25, 10, 25, 10, 25, 10, 25}, { 28, 12, 28, 12, 28, 12, 28, 12}, {-1, -3, -1, -3, -1, -3, -1, -3} },
    {{ -3, -3, -3, -3, -3, -3, -3, -3}, {  9, 26,  9, 26,  9, 26,  9, 26}, { 27, 14, 27, 14, 27, 14, 27, 14}, {-3, -3, -3, -3, -3, -3, -3, -3} },
    {{ -2, -4, -2, -4, -2, -4, -2, -4}, {  8, 25,  8, 25,  8, 25,  8, 25}, { 28, 14, 28, 14, 28, 14, 28, 14}, {-1, -4, -1, -4, -1, -4, -1, -4} },
    {{ -2, -4, -2, -4, -2, -4, -2, -4}, {  7, 24,  7, 24,  7, 24,  7, 24}, { 28, 16, 28, 16, 28, 16, 28, 16}, {-1, -4, -1, -4, -1, -4, -1, -4} },
    {{ -2, -4, -2, -4, -2, -4, -2, -4}, {  7, 23,  7, 23,  7, 23,  7, 23}, { 29, 16, 29, 16, 29, 16, 29, 16}, {-1, -4, -1, -4, -1, -4, -1, -4} },
    {{ -2, -4, -2, -4, -2, -4, -2, -4}, {  6, 22,  6, 22,  6, 22,  6, 22}, { 29, 18, 29, 18, 29, 18, 29, 18}, {-1, -4, -1, -4, -1, -4, -1, -4} },
    {{ -2, -4, -2, -4, -2, -4, -2, -4}, {  5, 21,  5, 21,  5, 21,  5, 21}, { 29, 19, 29, 19, 29, 19, 29, 19}, { 0, -4,  0, -4,  0, -4,  0, -4} }},

    // Affine
    {{{  0, 0,   0, 0,  0, 0,  0, 0}, {  0, 64,  0, 64,  0, 64,  0, 64}, {  0,   0,  0,   0,  0,   0,  0,   0 }, { 0,  0,  0,  0,  0,  0,  0,  0} },
    {{  0, 1,   0, 1,  0, 1,  0, 1}, {  -3, 63, -3, 63, -3, 63, -3, 63}, {  4,  -2,  4,  -2,  4,  -2,  4,  -2 }, { 1,  0,  1,  0,  1,  0,  1,  0} },
    {{  0, 1,   0, 1,  0, 1,  0, 1}, {  -5, 62, -5, 62, -5, 62, -5, 62}, {  8,  -3,  8,  -3,  8,  -3,  8,  -3 }, { 1,  0,  1,  0,  1,  0,  1,  0} },
    {{  0, 2,   0, 2,  0, 2,  0, 2}, {  -8, 60, -8, 60, -8, 60, -8, 60}, { 13,  -4, 13,  -4, 13,  -4, 13,  -4 }, { 1,  0,  1,  0,  1,  0,  1,  0} },
    {{  0, 3,   0, 3,  0, 3,  0, 3}, { -10, 58,-10, 58,-10, 58,-10, 58}, { 17,  -5, 17,  -5, 17,  -5, 17,  -5 }, { 1,  0,  1,  0,  1,  0,  1,  0} }, //1/4
    {{  0, 3,   0, 3,  0, 3,  0, 3}, { -11, 52,-11, 52,-11, 52,-11, 52}, { 26,  -8, 26,  -8, 26,  -8, 26,  -8 }, { 2,  0,  2,  0,  2,  0,  2,  0} },
    {{  0, 2,   0, 2,  0, 2,  0, 2}, {  -9, 47, -9, 47, -9, 47, -9, 47}, { 31, -10, 31, -10, 31, -10, 31, -10 }, { 3,  0,  3,  0,  3,  0,  3,  0} },
    {{  0, 3,   0, 3,  0, 3,  0, 3}, { -11, 45,-11, 45,-11, 45,-11, 45}, { 34, -10, 34, -10, 34, -10, 34, -10 }, { 3,  0,  3,  0,  3,  0,  3,  0} },
    {{  0, 3,   0, 3,  0, 3,  0, 3}, { -11, 40,-11, 40,-11, 40,-11, 40}, { 40, -11, 40, -11, 40, -11, 40, -11}, {  3,  0,   3,  0,   3,  0,   3,  0} }, //1/2
    {{  0, 3,   0, 3,  0, 3,  0, 3}, { -10, 34,-10, 34,-10, 34,-10, 34}, { 45, -11, 45, -11, 45, -11, 45, -11}, {  3,  0,   3,  0,   3,  0,   3,  0} },
    {{  0, 3,   0, 3,  0, 3,  0, 3}, { -10, 31,-10, 31,-10, 31,-10, 31}, { 47,  -9, 47,  -9, 47,  -9, 47,  -9}, {  2,  0,   2,  0,   2,  0,   2,  0} },
    {{  0, 2,   0, 2,  0, 2,  0, 2}, {  -8, 26, -8, 26, -8, 26, -8, 26}, { 52, -11, 52, -11, 52, -11, 52, -11}, {  3,  0,   3,  0,   3,  0,   3,  0} },
    {{  0, 1,   0, 1,  0, 1,  0, 1}, {  -5, 17, -5, 17, -5, 17, -5, 17}, { 58, -10, 58, -10, 58, -10, 58, -10}, {  3,  0,   3,  0,   3,  0,   3,  0} }, //3/4
    {{  0, 1,   0, 1,  0, 1,  0, 1}, {  -4, 13, -4, 13, -4, 13, -4, 13}, { 60,  -8, 60,  -8, 60,  -8, 60,  -8}, {  2,  0,   2,  0,   2,  0,   2,  0} },
    {{  0, 1,   0, 1,  0, 1,  0, 1}, {  -3,  8, -3,  8, -3,  8, -3,  8}, { 62,  -5, 62,  -5, 62,  -5, 62,  -5}, {  1,  0,   1,  0,   1,  0,   1,  0} },
    {{  0, 1,   0, 1,  0, 1,  0, 1}, {  -2,  4, -2,  4, -2,  4, -2,  4}, { 63,  -3, 63,  -3, 63,  -3, 63,  -3}, {  1,  0,   1,  0,   1,  0,   1,  0} }},
    // 1.5x
    {{{  0, -6,0, -6, 0, -6, 0, -6}, { 17, 42, 17, 42, 17, 42, 17, 42}, { 17, -5, 17, -5, 17, -5, 17, -5}, { -1,  0, -1,  0, -1,  0, -1,  0} },
      {{  0, -5,0, -5, 0, -5, 0, -5}, { 15, 41, 15, 41, 15, 41, 15, 41}, { 19, -5, 19, -5, 19, -5, 19, -5}, { -1,  0, -1,  0, -1,  0, -1,  0} },
      {{  0, -5,0, -5, 0, -5, 0, -5}, { 13, 40, 13, 40, 13, 40, 13, 40}, { 21, -4, 21, -4, 21, -4, 21, -4}, { -1,  0, -1,  0, -1,  0, -1,  0} },
      {{  0, -5,0, -5, 0, -5, 0, -5}, { 11, 39, 11, 39, 11, 39, 11, 39}, { 24, -4, 24, -4, 24, -4, 24, -4}, { -1,  0, -1,  0, -1,  0, -1,  0} },
      {{  0, -5,0, -5, 0, -5, 0, -5}, {  9, 38,  9, 38,  9, 38,  9, 38}, { 26, -3, 26, -3, 26, -3, 26, -3}, { -1,  0, -1,  0, -1,  0, -1,  0} },
      {{  0, -5,0, -5, 0, -5, 0, -5}, {  7, 38,  7, 38,  7, 38,  7, 38}, { 28, -2, 28, -2, 28, -2, 28, -2}, { -2,  0, -2,  0, -2,  0, -2,  0} },
      {{  0, -4,0, -4, 0, -4, 0, -4}, {  5, 36,  5, 36,  5, 36,  5, 36}, { 30, -1, 30, -1, 30, -1, 30, -1}, { -2,  0, -2,  0, -2,  0, -2,  0} },
      {{  0, -3,0, -3, 0, -3, 0, -3}, {  3, 35,  3, 35,  3, 35,  3, 35}, { 32,  0, 32,  0, 32,  0, 32,  0}, { -3,  0, -3,  0, -3,  0, -3,  0} },
      {{  0, -3,0, -3, 0, -3, 0, -3}, {  2, 33,  2, 33,  2, 33,  2, 33}, { 33,  2, 33,  2, 33,  2, 33,  2}, { -3,  0, -3,  0, -3,  0, -3,  0} },
      {{  0, -3,0, -3, 0, -3, 0, -3}, {  0, 32,  0, 32,  0, 32,  0, 32}, { 35,  3, 35,  3, 35,  3, 35,  3}, { -3,  0, -3,  0, -3,  0, -3,  0} },
      {{  0, -2,0, -2, 0, -2, 0, -2}, { -1, 30, -1, 30, -1, 30, -1, 30}, { 36,  5, 36,  5, 36,  5, 36,  5}, { -4,  0, -4,  0, -4,  0, -4,  0} },
      {{  0, -2,0, -2, 0, -2, 0, -2}, { -2, 28, -2, 28, -2, 28, -2, 28}, { 38,  7, 38,  7, 38,  7, 38,  7}, { -5,  0, -5,  0, -5,  0, -5,  0} },
      {{  0, -1,0, -1, 0, -1, 0, -1}, { -3, 26, -3, 26, -3, 26, -3, 26}, { 38,  9, 38,  9, 38,  9, 38,  9}, { -5,  0, -5,  0, -5,  0, -5,  0} },
      {{  0, -1,0, -1, 0, -1, 0, -1}, { -4, 24, -4, 24, -4, 24, -4, 24}, { 39, 11, 39, 11, 39, 11, 39, 11}, { -5,  0, -5,  0, -5,  0, -5,  0} },
      {{  0, -1,0, -1, 0, -1, 0, -1}, { -4, 21, -4, 21, -4, 21, -4, 21}, { 40, 13, 40, 13, 40, 13, 40, 13}, { -5,  0, -5,  0, -5,  0, -5,  0} },
      {{  0, -1,0, -1, 0, -1, 0, -1}, { -5, 19, -5, 19, -5, 19, -5, 19}, { 41, 15, 41, 15, 41, 15, 41, 15}, { -5,  0, -5,  0, -5,  0, -5,  0} }},

    // 2x
    {{ { 0, -2, 0, -2, 0, -2, 0, -2}, { 20, 28,20, 28,20, 28,20, 28}, {20,  2,20,  2,20,  2,20,  2}, {-4,  0, -4,  0, -4,  0, -4,  0} },
      { { 0, -4, 0, -4, 0, -4, 0, -4}, { 19, 29,19, 29,19, 29,19, 29}, {21,  5,21,  5,21,  5,21,  5}, {-6,  0, -6,  0, -6,  0, -6,  0} },
      { { 0, -5, 0, -5, 0, -5, 0, -5}, { 18, 29,18, 29,18, 29,18, 29}, {22,  6,22,  6,22,  6,22,  6}, {-6,  0, -6,  0, -6,  0, -6,  0} },
      { { 0, -5, 0, -5, 0, -5, 0, -5}, { 16, 29,16, 29,16, 29,16, 29}, {23,  7,23,  7,23,  7,23,  7}, {-6,  0, -6,  0, -6,  0, -6,  0} },
      { { 0, -5, 0, -5, 0, -5, 0, -5}, { 16, 28,16, 28,16, 28,16, 28}, {24,  7,24,  7,24,  7,24,  7}, {-6,  0, -6,  0, -6,  0, -6,  0} },
      { { 0, -5, 0, -5, 0, -5, 0, -5}, { 14, 28,14, 28,14, 28,14, 28}, {25,  8,25,  8,25,  8,25,  8}, {-6,  0, -6,  0, -6,  0, -6,  0} },
      { { 0, -6, 0, -6, 0, -6, 0, -6}, { 14, 27,14, 27,14, 27,14, 27}, {26,  9,26,  9,26,  9,26,  9}, {-6,  0, -6,  0, -6,  0, -6,  0} },
      { { 0, -4, 0, -4, 0, -4, 0, -4}, { 12, 28,12, 28,12, 28,12, 28}, {25, 10,25, 10,25, 10,25, 10}, {-7,  0, -7,  0, -7,  0, -7,  0} },
      { { 0, -6, 0, -6, 0, -6, 0, -6}, { 11, 27,11, 27,11, 27,11, 27}, {27, 11,27, 11,27, 11,27, 11}, {-6,  0, -6,  0, -6,  0, -6,  0} },
      { { 0, -7, 0, -7, 0, -7, 0, -7}, { 10, 25,10, 25,10, 25,10, 25}, {28, 12,28, 12,28, 12,28, 12}, {-4,  0, -4,  0, -4,  0, -4,  0} },
      { { 0, -6, 0, -6, 0, -6, 0, -6}, {  9, 26,9, 26,9, 26,9, 26}, {27, 14,27, 14,27, 14,27, 14}, {-6,  0, -6,  0, -6,  0, -6,  0} },
      { { 0, -6, 0, -6, 0, -6, 0, -6}, {  8, 25,8, 25,8, 25,8, 25}, {28, 14,28, 14,28, 14,28, 14}, {-5,  0, -5,  0, -5,  0, -5,  0} },
      { { 0, -6, 0, -6, 0, -6, 0, -6}, {  7, 24,7, 24,7, 24,7, 24}, {28, 16,28, 16,28, 16,28, 16}, {-5,  0, -5,  0, -5,  0, -5,  0} },
      { { 0, -6, 0, -6, 0, -6, 0, -6}, {  7, 23,7, 23,7, 23,7, 23}, {29, 16,29, 16,29, 16,29, 16}, {-5,  0, -5,  0, -5,  0, -5,  0} },
      { { 0, -6, 0, -6, 0, -6, 0, -6}, {  6, 22,6, 22,6, 22,6, 22}, {29, 18,29, 18,29, 18,29, 18}, {-5,  0, -5,  0, -5,  0, -5,  0} },
      { { 0, -6, 0, -6, 0, -6, 0, -6}, {  5, 21,5, 21,5, 21,5, 21}, {29, 19,29, 19,29, 19,29, 19}, {-4,  0, -4,  0, -4,  0, -4,  0} }}
};


DECLARE_ALIGNED(16, const int16_t, oh_hevc_qpel_filters_avx2[16][4][16]) = {
#if 0
  { { 0, 0, 0, 0, 0, 0, 0, 0},
  {0, 64, 0, 64, 0, 64, 0, 64},
  {0,   0, 0,   0, 0,   0, 0,   0},
  {0,  0 , 0,  0 , 0,  0 , 0,  0 }},
#endif
  { { 0, 1, 0, 1, 0, 1, 0, 1 , 0, 1, 0, 1, 0, 1, 0, 1 },
    { -3, 63, -3, 63, -3, 63, -3, 63 , -3, 63, -3, 63, -3, 63, -3, 63 },
    { 4, -2, 4, -2, 4, -2, 4, -2 , 4, -2, 4, -2, 4, -2, 4, -2 },
    { 1, 0, 1, 0, 1, 0, 1, 0 , 1, 0, 1, 0, 1, 0, 1, 0 } },
  { { -1, 2, -1, 2, -1, 2, -1, 2 , -1, 2, -1, 2, -1, 2, -1, 2 },
    { -5, 62, -5, 62, -5, 62, -5, 62 , -5, 62, -5, 62, -5, 62, -5, 62 },
    { 8, -3, 8, -3, 8, -3, 8, -3 , 8, -3, 8, -3, 8, -3, 8, -3 },
    { 1, 0, 1, 0, 1, 0, 1, 0 , 1, 0, 1, 0, 1, 0, 1, 0 } },
  { { -1, 3, -1, 3, -1, 3, -1, 3 , -1, 3, -1, 3, -1, 3, -1, 3 },
    { -8, 60, -8, 60, -8, 60, -8, 60 , -8, 60, -8, 60, -8, 60, -8, 60 },
    { 13, -4, 13, -4, 13, -4, 13, -4 , 13, -4, 13, -4, 13, -4, 13, -4 },
    { 1, 0, 1, 0, 1, 0, 1, 0 , 1, 0, 1, 0, 1, 0, 1, 0 } },
  { { -1, 4, -1, 4, -1, 4, -1, 4 , -1, 4, -1, 4, -1, 4, -1, 4 },
    { -10, 58, -10, 58, -10, 58, -10, 58 , -10, 58, -10, 58, -10, 58, -10, 58 },
    { 17, -5, 17, -5, 17, -5, 17, -5 , 17, -5, 17, -5, 17, -5, 17, -5 },
    { 1, 0, 1, 0, 1, 0, 1, 0 , 1, 0, 1, 0, 1, 0, 1, 0 } },
  { { -1, 4, -1, 4, -1, 4, -1, 4 , -1, 4, -1, 4, -1, 4, -1, 4 },
    { -11, 52, -11, 52, -11, 52, -11, 52 , -11, 52, -11, 52, -11, 52, -11, 52 },
    { 26, -8, 26, -8, 26, -8, 26, -8 , 26, -8, 26, -8, 26, -8, 26, -8 },
    { 3, -1, 3, -1, 3, -1, 3, -1 , 3, -1, 3, -1, 3, -1, 3, -1 } },
  { { -1, 3, -1, 3, -1, 3, -1, 3 , -1, 3, -1, 3, -1, 3, -1, 3 },
    { -9, 47, -9, 47, -9, 47, -9, 47 , -9, 47, -9, 47, -9, 47, -9, 47 },
    { 31, -10, 31, -10, 31, -10, 31, -10 , 31, -10, 31, -10, 31, -10, 31, -10 },
    { 4, -1, 4, -1, 4, -1, 4, -1 , 4, -1, 4, -1, 4, -1, 4, -1 } },
  { { -1, 4, -1, 4, -1, 4, -1, 4 , -1, 4, -1, 4, -1, 4, -1, 4 },
    { -11, 45, -11, 45, -11, 45, -11, 45 , -11, 45, -11, 45, -11, 45, -11, 45 },
    { 34, -10, 34, -10, 34, -10, 34, -10 , 34, -10, 34, -10, 34, -10, 34, -10 },
    { 4, -1, 4, -1, 4, -1, 4, -1 , 4, -1, 4, -1, 4, -1, 4, -1 } },
  { { -1, 4, -1, 4, -1, 4, -1, 4 , -1, 4, -1, 4, -1, 4, -1, 4 },
    { -11, 40, -11, 40, -11, 40, -11, 40 , -11, 40, -11, 40, -11, 40, -11, 40 },
    { 40, -11, 40, -11, 40, -11, 40, -11 , 40, -11, 40, -11, 40, -11, 40, -11 },
    { 4, -1, 4, -1, 4, -1, 4, -1 , 4, -1, 4, -1, 4, -1, 4, -1 } },
  { { -1, 4, -1, 4, -1, 4, -1, 4 , -1, 4, -1, 4, -1, 4, -1, 4 },
    { -10, 34, -10, 34, -10, 34, -10, 34 , -10, 34, -10, 34, -10, 34, -10, 34 },
    { 45, -11, 45, -11, 45, -11, 45, -11 , 45, -11, 45, -11, 45, -11, 45, -11 },
    { 4, -1, 4, -1, 4, -1, 4, -1 , 4, -1, 4, -1, 4, -1, 4, -1 } },
  { { -1, 4, -1, 4, -1, 4, -1, 4 , -1, 4, -1, 4, -1, 4, -1, 4 },
    { -10, 31, -10, 31, -10, 31, -10, 31 , -10, 31, -10, 31, -10, 31, -10, 31 },
    { 47, -9, 47, -9, 47, -9, 47, -9 , 47, -9, 47, -9, 47, -9, 47, -9 },
    { 3, -1, 3, -1, 3, -1, 3, -1 , 3, -1, 3, -1, 3, -1, 3, -1 } },
  { { -1, 3, -1, 3, -1, 3, -1, 3 , -1, 3, -1, 3, -1, 3, -1, 3 },
    { -8, 26, -8, 26, -8, 26, -8, 26 , -8, 26, -8, 26, -8, 26, -8, 26 },
    { 52, -11, 52, -11, 52, -11, 52, -11 , 52, -11, 52, -11, 52, -11, 52, -11 },
    { 4, -1, 4, -1, 4, -1, 4, -1 , 4, -1, 4, -1, 4, -1, 4, -1 } },
  { { 0, 1, 0, 1, 0, 1, 0, 1 , 0, 1, 0, 1, 0, 1, 0, 1 },
    { -5, 17, -5, 17, -5, 17, -5, 17 , -5, 17, -5, 17, -5, 17, -5, 17 },
    { 58, -10, 58, -10, 58, -10, 58, -10 , 58, -10, 58, -10, 58, -10, 58, -10 },
    { 4, -1, 4, -1, 4, -1, 4, -1 , 4, -1, 4, -1, 4, -1, 4, -1 } },
  { { 0, 1, 0, 1, 0, 1, 0, 1 , 0, 1, 0, 1, 0, 1, 0, 1 },
    { -4, 13, -4, 13, -4, 13, -4, 13 , -4, 13, -4, 13, -4, 13, -4, 13 },
    { 60, -8, 60, -8, 60, -8, 60, -8 , 60, -8, 60, -8, 60, -8, 60, -8 },
    { 3, -1, 3, -1, 3, -1, 3, -1 , 3, -1, 3, -1, 3, -1, 3, -1 } },
  { { 0, 1, 0, 1, 0, 1, 0, 1 , 0, 1, 0, 1, 0, 1, 0, 1 },
    { -3, 8, -3, 8, -3, 8, -3, 8 , -3, 8, -3, 8, -3, 8, -3, 8 },
    { 62, -5, 62, -5, 62, -5, 62, -5 , 62, -5, 62, -5, 62, -5, 62, -5 },
    { 2, -1, 2, -1, 2, -1, 2, -1 , 2, -1, 2, -1, 2, -1, 2, -1 } },
  { { 0, 1, 0, 1, 0, 1, 0, 1 , 0, 1, 0, 1, 0, 1, 0, 1 },
    { -2, 4, -2, 4, -2, 4, -2, 4 , -2, 4, -2, 4, -2, 4, -2, 4 },
    { 63, -3, 63, -3, 63, -3, 63, -3 , 63, -3, 63, -3, 63, -3, 63, -3 },
    { 1, 0, 1, 0, 1, 0, 1, 0 , 1, 0, 1, 0, 1, 0, 1, 0 } },

    //Hpel for amvr
    { { 0, 3, 0, 3, 0, 3, 0, 3 , 0, 3, 0, 3, 0, 3, 0, 3 },
    { 9, 20, 9, 20, 9, 20, 9, 20 , 9, 20, 9, 20, 9, 20, 9, 20 },
    { 20, 9, 20, 9, 20, 9, 20, 9 , 20, 9, 20, 9, 20, 9, 20, 9 },
    { 3, 0, 3, 0, 3, 0, 3, 0 , 3, 0, 3, 0, 3, 0, 3, 0 } }
};

DECLARE_ALIGNED(16, static const int16_t, ov_mc_filters_4_avx2[15][4][8]) =
{
    {{  0, 1,  0, 1,  0, 1,  0, 1},
        {-3, 63, -3, 63, -3, 63, -3, 63},
        {4,  -2, 4,  -2, 4,  -2, 4,  -2},
        {1,  0 , 1,  0 , 1,  0 , 1,  0 }},

    {{  0, 1,  0, 1,  0, 1,  0, 1},
        {-5, 62, -5, 62, -5, 62, -5, 62},
        {8,  -3, 8,  -3, 8,  -3, 8,  -3},
        {1,  0 , 1,  0 , 1,  0 , 1,  0 }},

    {{  0, 2,  0, 2,  0, 2,  0, 2},
        {-8, 60, -8, 60, -8, 60, -8, 60},
        {13,  -4, 13,  -4, 13,  -4, 13,  -4},
        {1,  0 , 1,  0 , 1,  0 , 1,  0 }},

    {{  0, 3,  0, 3,  0, 3,  0, 3},
        {-10, 58, -10, 58, -10, 58, -10, 58},
        {17,  -5, 17,  -5, 17,  -5, 17,  -5},
        {1,  0 , 1,  0 , 1,  0 , 1,  0 }},
    //1/4
    {{  0, 3,  0, 3,  0, 3,  0, 3},
        {-11, 52, -11, 52, -11, 52, -11, 52},
        {26,  -8, 26,  -8, 26,  -8, 26,  -8},
        {2,  0 , 2,  0 , 2,  0 , 2,  0 }},

    {{  0, 2,  0, 2,  0, 2,  0, 2},
        {-9, 47, -9, 47, -9, 47, -9, 47},
        {31, -10, 31, -10, 31, -10, 31, -10},
        {3,  0, 3,  0, 3,  0, 3,  0    }},

    {{  0, 3,  0, 3,  0, 3,  0, 3},
        {-11, 45, -11, 45, -11, 45, -11, 45},
        {34, -10, 34, -10, 34, -10, 34, -10},
        {3,  0 , 3,  0 , 3,  0 , 3,  0 }},

    {{  0, 3,  0, 3,  0, 3,  0, 3},
        {-11, 40, -11, 40, -11, 40, -11, 40},
        {40, -11, 40, -11, 40, -11, 40, -11},
        {3,  0 , 3,  0 , 3,  0 , 3,  0 }},
    //1/2
    {{  0, 3,  0, 3,  0, 3,  0, 3},
        {-10, 34, -10, 34, -10, 34, -10, 34},
        {45, -11, 45, -11, 45, -11, 45, -11},
        {3,  0 , 3,  0 , 3,  0 , 3,  0 }},

    {{  0, 3,  0, 3,  0, 3,  0, 3},
        {-10, 31, -10, 31, -10, 31, -10, 31},
        {47,  -9, 47,  -9, 47,  -9, 47,  -9},
        {2,  0 , 2,  0 , 2,  0 , 2,  0 }},

    {{  0, 2,  0, 2,  0, 2,  0, 2},
        {-8, 26, -8, 26, -8, 26, -8, 26},
        {52, -11, 52, -11, 52, -11, 52, -11},
        {3,  0 , 3,  0 , 3,  0 , 3,  0 }},

    {{  0, 1,  0, 1,  0, 1,  0, 1},
        {-5, 17, -5, 17, -5, 17, -5, 17},
        {58, -10, 58, -10, 58, -10, 58, -10},
        {3,  0 , 3,  0 , 3,  0 , 3,  0 }},
    //3/4
    {{  0, 1,  0, 1,  0, 1,  0, 1},
        {-4, 13, -4, 13, -4, 13, -4, 13},
        {60,  -8, 60,  -8, 60,  -8, 60,  -8},
        {2,  0 , 2,  0 , 2,  0 , 2,  0 }},

    {{  0, 1,  0, 1,  0, 1,  0, 1},
        {-3,  8, -3,  8, -3,  8, -3,  8},
        {62,  -5, 62,  -5, 62,  -5, 62,  -5},
        {1,  0 , 1,  0 , 1,  0 , 1,  0 }},

    {{  0, 1,  0, 1,  0, 1,  0, 1},
        {-2,  4, -2,  4, -2,  4, -2,  4},
        {63,  -3, 63,  -3, 63,  -3, 63,  -3},
        {1,  0,  1,  0,  1,  0,  1,  0}}
};

static const int16_t ov_bilinear_filters_4[15][16] =
{
  /*{ 16,  0, },*/
  { 15,  1, 15,  1, 15,  1, 15,  1, 15,  1, 15,  1, 15,  1, 15,  1, },
  { 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, 14,  2, },
  { 13,  3, 13,  3, 13,  3, 13,  3, 13,  3, 13,  3, 13,  3, 13,  3, },
  { 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, 12,  4, },
  { 11,  5, 11,  5, 11,  5, 11,  5, 11,  5, 11,  5, 11,  5, 11,  5, },
  { 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, 10,  6, },
  { 9,   7,  9,  7,  9,  7,  9,  7, 9,   7,  9,  7,  9,  7,  9,  7, },
  { 8,   8,  8,  8,  8,  8,  8,  8, 8,   8,  8,  8,  8,  8,  8,  8, },
  { 7,   9,  7,  9,  7,  9,  7,  9, 7,   9,  7,  9,  7,  9,  7,  9, },
  { 6,  10,  6, 10,  6, 10,  6, 10, 6,  10,  6, 10,  6, 10,  6, 10, },
  { 5,  11,  5, 11,  5, 11,  5, 11, 5,  11,  5, 11,  5, 11,  5, 11, },
  { 4,  12,  4, 12,  4, 12,  4, 12, 4,  12,  4, 12,  4, 12,  4, 12, },
  { 3,  13,  3, 13,  3, 13,  3, 13, 3,  13,  3, 13,  3, 13,  3, 13, },
  { 2,  14,  2, 14,  2, 14,  2, 14, 2,  14,  2, 14,  2, 14,  2, 14, },
  { 1,  15,  1, 15,  1, 15,  1, 15, 1,  15,  1, 15,  1, 15,  1, 15, }
};

static void
oh_hevc_put_hevc_bi0_pel_pixels16_10_avx2(int16_t* dst,
                                        const uint16_t* _src,
                                        ptrdiff_t _srcstride,
                                        int height,
                                        intptr_t mx,
                                        intptr_t my,
                                        int width)
{
  int x, y;
  __m256i x1;
  const uint16_t* src = _src;
  const int srcstride = _srcstride;
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x1 = _mm256_slli_epi16(x1, 14 - 10);
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += 128;
  }
}

static void
oh_hevc_put_hevc_bi1_pel_pixels16_10_avx2(uint16_t* _dst,
                                        ptrdiff_t _dststride,
                                        const uint16_t* _src,
                                        ptrdiff_t _srcstride,
                                        const int16_t* src2,
                                        int height,
                                        intptr_t mx,
                                        intptr_t my,
                                        int width)
{
  int x, y;
  __m256i x1;
  const uint16_t* src = _src;
  const int srcstride = _srcstride;
  const __m256i offset = _mm256_set1_epi16(1 << 10);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      __m256i r5;
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x1 = _mm256_slli_epi16(x1, 14 - 10);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      x1 = _mm256_adds_epi16(x1, r5);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    src2 += 128;
    dst += dststride;
  }
}

static void
oh_hevc_put_hevc_bi0_epel_h16_10_avx2(int16_t* dst,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2;
  uint16_t* src = ((uint16_t*)_src) - 1;
  ptrdiff_t srcstride = _srcstride;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][1]);
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      x1 = _mm256_packs_epi32(x1, t1);
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += 128;
  }
}

static void
oh_hevc_put_hevc_bi1_epel_h16_10_avx2(uint16_t* _dst,
                                    ptrdiff_t _dststride,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    const int16_t* src2,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2;
  uint16_t* src = ((uint16_t*)_src) - 1;
  ptrdiff_t srcstride = _srcstride;
  const __m256i offset = _mm256_set1_epi16(1 << 10);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][1]);
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      __m256i r5;
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      x1 = _mm256_packs_epi32(x1, t1);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      x1 = _mm256_adds_epi16(x1, r5);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    src2 += 128;
    dst += dststride;
  }
}

static void
oh_hevc_put_hevc_uni_epel_h16_10_avx2(uint16_t* _dst,
                                    ptrdiff_t _dststride,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2;
  uint16_t* src = ((uint16_t*)_src) - 1;
  ptrdiff_t srcstride = _srcstride;
  const __m256i offset = _mm256_set1_epi16(1 << (10 + 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][1]);
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      x1 = _mm256_packs_epi32(x1, t1);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += dststride;
  }
}

static void
oh_hevc_put_hevc_bi0_epel_v16_10_avx2(int16_t* dst,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2;
  ptrdiff_t srcstride = _srcstride;
  uint16_t* src = ((uint16_t*)_src) - srcstride;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][1]);
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      x1 = _mm256_packs_epi32(x1, t1);
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += 128;
  }
}

static void
oh_hevc_put_hevc_bi1_epel_v16_10_avx2(uint16_t* _dst,
                                    ptrdiff_t _dststride,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    const int16_t* src2,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2;
  ptrdiff_t srcstride = _srcstride;
  uint16_t* src = ((uint16_t*)_src) - srcstride;
  const __m256i offset = _mm256_set1_epi16(1 << 10);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][1]);
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      __m256i r5;
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      x1 = _mm256_packs_epi32(x1, t1);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      x1 = _mm256_adds_epi16(x1, r5);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    src2 += 128;
    dst += dststride;
  }
}

static void
oh_hevc_put_hevc_uni_epel_v16_10_avx2(uint16_t* _dst,
                                    ptrdiff_t _dststride,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2;
  ptrdiff_t srcstride = _srcstride;
  uint16_t* src = ((uint16_t*)_src) - srcstride;
  const __m256i offset = _mm256_set1_epi16(1 << (10 + 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][1]);
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      x1 = _mm256_packs_epi32(x1, t1);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += dststride;
  }
}

static void
oh_hevc_put_hevc_bi0_epel_hv16_10_avx2(int16_t* dst,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2, f3, f4, r1, r2, r3, r4;
  int16_t* dst_bis = dst;
  uint16_t *src_bis, *src = ((uint16_t*)_src) - 1;
  ptrdiff_t srcstride = _srcstride;
  src -= 1 * srcstride;
  src_bis = src;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][1]);
  f3 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][0]);
  f4 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][1]);
  for (x = 0; x < width; x += 16) {
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r1 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r2 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r3 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    for (y = 0; y < height; y++) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      r4 = _mm256_packs_epi32(x1, t1);
      src += srcstride;
      t1 = _mm256_unpackhi_epi16(r1, r2);
      x1 = _mm256_unpacklo_epi16(r1, r2);
      t2 = _mm256_unpackhi_epi16(r3, r4);
      x2 = _mm256_unpacklo_epi16(r3, r4);
      x1 = _mm256_madd_epi16(x1, f3);
      t1 = _mm256_madd_epi16(t1, f3);
      x2 = _mm256_madd_epi16(x2, f4);
      t2 = _mm256_madd_epi16(t2, f4);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 6);
      x1 = _mm256_srai_epi32(x1, 6);
      x1 = _mm256_packs_epi32(x1, t1);
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
      dst += 128;
      r1 = r2;
      r2 = r3;
      r3 = r4;
    }
    src = src_bis;
    dst = dst_bis;
  }
}

static void
oh_hevc_put_hevc_bi1_epel_hv16_10_avx2(uint16_t* _dst,
                                     ptrdiff_t _dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     const int16_t* src2,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2, f3, f4, r1, r2, r3, r4;
  const __m256i offset = _mm256_set1_epi16(1 << 10);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  const int16_t* src2_bis = src2;
  uint16_t* dst_bis = dst;
  uint16_t *src_bis, *src = ((uint16_t*)_src) - 1;
  ptrdiff_t srcstride = _srcstride;
  src -= 1 * srcstride;
  src_bis = src;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][1]);
  f3 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][0]);
  f4 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][1]);
  for (x = 0; x < width; x += 16) {
    __m256i r5;
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r1 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r2 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r3 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    for (y = 0; y < height; y++) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      r4 = _mm256_packs_epi32(x1, t1);
      src += srcstride;
      t1 = _mm256_unpackhi_epi16(r1, r2);
      x1 = _mm256_unpacklo_epi16(r1, r2);
      t2 = _mm256_unpackhi_epi16(r3, r4);
      x2 = _mm256_unpacklo_epi16(r3, r4);
      x1 = _mm256_madd_epi16(x1, f3);
      t1 = _mm256_madd_epi16(t1, f3);
      x2 = _mm256_madd_epi16(x2, f4);
      t2 = _mm256_madd_epi16(t2, f4);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 6);
      x1 = _mm256_srai_epi32(x1, 6);
      x1 = _mm256_packs_epi32(x1, t1);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      x1 = _mm256_adds_epi16(x1, r5);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
      src2 += 128;
      dst += dststride;
      r1 = r2;
      r2 = r3;
      r3 = r4;
    }
    src = src_bis;
    src2 = src2_bis;
    dst = dst_bis;
  }
}

static void
oh_hevc_put_hevc_uni_epel_hv16_10_avx2(uint16_t* _dst,
                                     ptrdiff_t _dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2, f3, f4, r1, r2, r3, r4;
  const __m256i offset = _mm256_set1_epi16(1 << (10 + 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  uint16_t* dst_bis = dst;
  uint16_t *src_bis, *src = ((uint16_t*)_src) - 1;
  ptrdiff_t srcstride = _srcstride;
  src -= 1 * srcstride;
  src_bis = src;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][1]);
  f3 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][0]);
  f4 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][1]);
  for (x = 0; x < width; x += 16) {
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r1 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r2 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r3 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    for (y = 0; y < height; y++) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      r4 = _mm256_packs_epi32(x1, t1);
      src += srcstride;
      t1 = _mm256_unpackhi_epi16(r1, r2);
      x1 = _mm256_unpacklo_epi16(r1, r2);
      t2 = _mm256_unpackhi_epi16(r3, r4);
      x2 = _mm256_unpacklo_epi16(r3, r4);
      x1 = _mm256_madd_epi16(x1, f3);
      t1 = _mm256_madd_epi16(t1, f3);
      x2 = _mm256_madd_epi16(x2, f4);
      t2 = _mm256_madd_epi16(t2, f4);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 6);
      x1 = _mm256_srai_epi32(x1, 6);
      x1 = _mm256_packs_epi32(x1, t1);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
      dst += dststride;
      r1 = r2;
      r2 = r3;
      r3 = r4;
    }
    src = src_bis;
    dst = dst_bis;
  }
}

#if 1
DECLARE_ALIGNED(16, static const int32_t, ov_mcp_filters_l[16][4]) = {
#if 0
  { { 0, 0, 0, 0, 0, 0, 0, 0},
  {0, 64, 0, 64, 0, 64, 0, 64},
  {0,   0, 0,   0, 0,   0, 0,   0},
  {0,  0 , 0,  0 , 0,  0 , 0,  0 }},
  { {  0, 1}, { -3, 63}, {  4,  -2}, { 1,  0} },
  { { -1, 2}, { -5, 62}, {  8,  -3}, { 1,  0} },
  { { -1, 3}, { -8, 60}, { 13,  -4}, { 1,  0} },
  { { -1, 4}, {-10, 58}, { 17,  -5}, { 1,  0} },
  { { -1, 4}, {-11, 52}, { 26,  -8}, { 3, -1} },
  { { -1, 3}, { -9, 47}, { 31, -10}, { 4, -1} },
  { { -1, 4}, {-11, 45}, { 34, -10}, { 4, -1} },
  { { -1, 4}, {-11, 40}, { 40, -11}, { 4, -1} },
  { { -1, 4}, {-10, 34}, { 45, -11}, { 4, -1} },
  { { -1, 4}, {-10, 31}, { 47,  -9}, { 3, -1} },
  { { -1, 3}, { -8, 26}, { 52, -11}, { 4, -1} },
  { {  0, 1}, { -5, 17}, { 58, -10}, { 4, -1} },
  { {  0, 1}, { -4, 13}, { 60,  -8}, { 3, -1} },
  { {  0, 1}, { -3,  8}, { 62,  -5}, { 2, -1} },
  { {  0, 1}, { -2,  4}, { 63,  -3}, { 1,  0} },

    //Hpel for amvr
    { { 0, 3}, { 9, 20}, { 20, 9}, { 3, 0} }
};
#endif
{0x00010000, 0x003ffffd, 0xfffe0004, 0x00000001},
{0x0002ffff, 0x003efffb, 0xfffd0008, 0x00000001},
{0x0003ffff, 0x003cfff8, 0xfffc000d, 0x00000001},
{0x0004ffff, 0x003afff6, 0xfffb0011, 0x00000001},
{0x0004ffff, 0x0034fff5, 0xfff8001a, 0xffff0003},
{0x0003ffff, 0x002ffff7, 0xfff6001f, 0xffff0004},
{0x0004ffff, 0x002dfff5, 0xfff60022, 0xffff0004},
{0x0004ffff, 0x0028fff5, 0xfff50028, 0xffff0004},
{0x0004ffff, 0x0022fff6, 0xfff5002d, 0xffff0004},
{0x0004ffff, 0x001ffff6, 0xfff7002f, 0xffff0003},
{0x0003ffff, 0x001afff8, 0xfff50034, 0xffff0004},
{0x00010000, 0x0011fffb, 0xfff6003a, 0xffff0004},
{0x00010000, 0x000dfffc, 0xfff8003c, 0xffff0003},
{0x00010000, 0x0008fffd, 0xfffb003e, 0xffff0002},
{0x00010000, 0x0004fffe, 0xfffd003f, 0x00000001},
{0x00030000, 0x00140009, 0x00090014, 0x00000003}
};
#endif

static void
oh_hevc_put_hevc_bi0_qpel_h16_10_avx2(int16_t* dst,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  int shift = 10 - 8;
  __m256i x1, x2, x3, x4, r1, r2, r3, r4, c1, c2, c3, c4, t1, t2;
  const uint16_t* src = _src;
  const int srcstride = _srcstride;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][3]);

  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * 1]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      t1 = _mm256_madd_epi16(t1, c1);
      t2 = _mm256_madd_epi16(t2, c2);
      r2 = _mm256_add_epi32(t1, t2);
      x1 = _mm256_madd_epi16(x1, c1);
      x2 = _mm256_madd_epi16(x2, c2);
      r1 = _mm256_add_epi32(x1, x2);
      x1 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 4 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      t1 = _mm256_madd_epi16(t1, c3);
      t2 = _mm256_madd_epi16(t2, c4);
      r4 = _mm256_add_epi32(t1, t2);
      x1 = _mm256_madd_epi16(x1, c3);
      x2 = _mm256_madd_epi16(x2, c4);
      r3 = _mm256_add_epi32(x1, x2);
      x1 = _mm256_add_epi32(r1, r3);
      x2 = _mm256_add_epi32(r2, r4);
      x1 = _mm256_srai_epi32(x1, shift);
      x2 = _mm256_srai_epi32(x2, shift);
      x1 = _mm256_packs_epi32(x1, x2);
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += 128;
  }
}

static void
oh_hevc_put_hevc_bi1_qpel_h16_10_avx2(uint16_t* _dst,
                                    ptrdiff_t _dststride,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    const int16_t* src2,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  int shift = 10 - 8;
  __m256i x1, x2, x3, x4, r1, r2, r3, r4, c1, c2, c3, c4, t1, t2;
  const uint16_t* src = _src;
  const int srcstride = _srcstride;
  const __m256i offset = _mm256_set1_epi16(1 << 10);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][3]);

  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      __m256i r5;
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * 1]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      t1 = _mm256_madd_epi16(t1, c1);
      t2 = _mm256_madd_epi16(t2, c2);
      r2 = _mm256_add_epi32(t1, t2);
      x1 = _mm256_madd_epi16(x1, c1);
      x2 = _mm256_madd_epi16(x2, c2);
      r1 = _mm256_add_epi32(x1, x2);
      x1 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 4 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      t1 = _mm256_madd_epi16(t1, c3);
      t2 = _mm256_madd_epi16(t2, c4);
      r4 = _mm256_add_epi32(t1, t2);
      x1 = _mm256_madd_epi16(x1, c3);
      x2 = _mm256_madd_epi16(x2, c4);
      r3 = _mm256_add_epi32(x1, x2);
      x1 = _mm256_add_epi32(r1, r3);
      x2 = _mm256_add_epi32(r2, r4);
      x1 = _mm256_srai_epi32(x1, shift);
      x2 = _mm256_srai_epi32(x2, shift);
      x1 = _mm256_packs_epi32(x1, x2);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      x1 = _mm256_adds_epi16(x1, r5);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    src2 += 128;
    dst += dststride;
  }
}

static void
oh_hevc_put_hevc_uni_qpel_h16_10_avx2(uint16_t* _dst,
                                    ptrdiff_t _dststride,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  int shift = 10 - 8;
  __m256i x1, x2, x3, x4, r1, r2, r3, r4, c1, c2, c3, c4, t1, t2;
  const uint16_t* src = _src;
  const int srcstride = _srcstride;
  const __m256i offset = _mm256_set1_epi16(1 << (10 + 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][3]);

  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * 1]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      t1 = _mm256_madd_epi16(t1, c1);
      t2 = _mm256_madd_epi16(t2, c2);
      r2 = _mm256_add_epi32(t1, t2);
      x1 = _mm256_madd_epi16(x1, c1);
      x2 = _mm256_madd_epi16(x2, c2);
      r1 = _mm256_add_epi32(x1, x2);
      x1 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 4 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      t1 = _mm256_madd_epi16(t1, c3);
      t2 = _mm256_madd_epi16(t2, c4);
      r4 = _mm256_add_epi32(t1, t2);
      x1 = _mm256_madd_epi16(x1, c3);
      x2 = _mm256_madd_epi16(x2, c4);
      r3 = _mm256_add_epi32(x1, x2);
      x1 = _mm256_add_epi32(r1, r3);
      x2 = _mm256_add_epi32(r2, r4);
      x1 = _mm256_srai_epi32(x1, shift);
      x2 = _mm256_srai_epi32(x2, shift);
      x1 = _mm256_packs_epi32(x1, x2);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += dststride;
  }
}

static void
oh_hevc_put_hevc_bi0_qpel_v16_10_avx2(int16_t* dst,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, x5, x6, x7, x8, x9, c1, c2, c3, c4;
  const uint16_t* src = _src;
  const int srcstride = _srcstride;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][3]);

  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * srcstride]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      x5 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x6 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x7 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      x8 = _mm256_loadu_si256((__m256i*)&src[x + 4 * srcstride]);
      x9 = x1;
      x1 = _mm256_unpacklo_epi16(x9, x2);
      x2 = _mm256_unpackhi_epi16(x9, x2);
      x9 = x3;
      x3 = _mm256_unpacklo_epi16(x9, x4);
      x4 = _mm256_unpackhi_epi16(x9, x4);
      x9 = x5;
      x5 = _mm256_unpacklo_epi16(x9, x6);
      x6 = _mm256_unpackhi_epi16(x9, x6);
      x9 = x7;
      x7 = _mm256_unpacklo_epi16(x9, x8);
      x8 = _mm256_unpackhi_epi16(x9, x8);
      x1 = _mm256_madd_epi16(x1, c1);
      x3 = _mm256_madd_epi16(x3, c2);
      x5 = _mm256_madd_epi16(x5, c3);
      x7 = _mm256_madd_epi16(x7, c4);
      x2 = _mm256_madd_epi16(x2, c1);
      x4 = _mm256_madd_epi16(x4, c2);
      x6 = _mm256_madd_epi16(x6, c3);
      x8 = _mm256_madd_epi16(x8, c4);
      x1 = _mm256_add_epi32(x1, x3);
      x3 = _mm256_add_epi32(x5, x7);
      x2 = _mm256_add_epi32(x2, x4);
      x4 = _mm256_add_epi32(x6, x8);
      x1 = _mm256_add_epi32(x1, x3);
      x2 = _mm256_add_epi32(x2, x4);
      x1 = _mm256_srai_epi32(x1, 2);
      x2 = _mm256_srai_epi32(x2, 2);
      x1 = _mm256_packs_epi32(x1, x2);
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += 128;
  }
}

static void
oh_hevc_put_hevc_bi1_qpel_v16_10_avx2(uint16_t* _dst,
                                    ptrdiff_t _dststride,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    const int16_t* src2,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, x5, x6, x7, x8, x9, c1, c2, c3, c4;
  const uint16_t* src = _src;
  const int srcstride = _srcstride;
  const __m256i offset = _mm256_set1_epi16(1 << 10);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][3]);

  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      __m256i r5;
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * srcstride]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      x5 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x6 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x7 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      x8 = _mm256_loadu_si256((__m256i*)&src[x + 4 * srcstride]);
      x9 = x1;
      x1 = _mm256_unpacklo_epi16(x9, x2);
      x2 = _mm256_unpackhi_epi16(x9, x2);
      x9 = x3;
      x3 = _mm256_unpacklo_epi16(x9, x4);
      x4 = _mm256_unpackhi_epi16(x9, x4);
      x9 = x5;
      x5 = _mm256_unpacklo_epi16(x9, x6);
      x6 = _mm256_unpackhi_epi16(x9, x6);
      x9 = x7;
      x7 = _mm256_unpacklo_epi16(x9, x8);
      x8 = _mm256_unpackhi_epi16(x9, x8);
      x1 = _mm256_madd_epi16(x1, c1);
      x3 = _mm256_madd_epi16(x3, c2);
      x5 = _mm256_madd_epi16(x5, c3);
      x7 = _mm256_madd_epi16(x7, c4);
      x2 = _mm256_madd_epi16(x2, c1);
      x4 = _mm256_madd_epi16(x4, c2);
      x6 = _mm256_madd_epi16(x6, c3);
      x8 = _mm256_madd_epi16(x8, c4);
      x1 = _mm256_add_epi32(x1, x3);
      x3 = _mm256_add_epi32(x5, x7);
      x2 = _mm256_add_epi32(x2, x4);
      x4 = _mm256_add_epi32(x6, x8);
      x1 = _mm256_add_epi32(x1, x3);
      x2 = _mm256_add_epi32(x2, x4);
      x1 = _mm256_srai_epi32(x1, 2);
      x2 = _mm256_srai_epi32(x2, 2);
      x1 = _mm256_packs_epi32(x1, x2);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      x1 = _mm256_adds_epi16(x1, r5);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    src2 += 128;
    dst += dststride;
  }
}

static void
oh_hevc_put_hevc_uni_qpel_v16_10_avx2(uint16_t* _dst,
                                    ptrdiff_t _dststride,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, x5, x6, x7, x8, x9, c1, c2, c3, c4;
  const uint16_t* src = _src;
  const int srcstride = _srcstride;
  const __m256i offset = _mm256_set1_epi16(1 << (10 + 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][3]);

  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * srcstride]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      x5 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x6 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x7 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      x8 = _mm256_loadu_si256((__m256i*)&src[x + 4 * srcstride]);
      x9 = x1;
      x1 = _mm256_unpacklo_epi16(x9, x2);
      x2 = _mm256_unpackhi_epi16(x9, x2);
      x9 = x3;
      x3 = _mm256_unpacklo_epi16(x9, x4);
      x4 = _mm256_unpackhi_epi16(x9, x4);
      x9 = x5;
      x5 = _mm256_unpacklo_epi16(x9, x6);
      x6 = _mm256_unpackhi_epi16(x9, x6);
      x9 = x7;
      x7 = _mm256_unpacklo_epi16(x9, x8);
      x8 = _mm256_unpackhi_epi16(x9, x8);
      x1 = _mm256_madd_epi16(x1, c1);
      x3 = _mm256_madd_epi16(x3, c2);
      x5 = _mm256_madd_epi16(x5, c3);
      x7 = _mm256_madd_epi16(x7, c4);
      x2 = _mm256_madd_epi16(x2, c1);
      x4 = _mm256_madd_epi16(x4, c2);
      x6 = _mm256_madd_epi16(x6, c3);
      x8 = _mm256_madd_epi16(x8, c4);
      x1 = _mm256_add_epi32(x1, x3);
      x3 = _mm256_add_epi32(x5, x7);
      x2 = _mm256_add_epi32(x2, x4);
      x4 = _mm256_add_epi32(x6, x8);
      x1 = _mm256_add_epi32(x1, x3);
      x2 = _mm256_add_epi32(x2, x4);
      x1 = _mm256_srai_epi32(x1, 2);
      x2 = _mm256_srai_epi32(x2, 2);
      x1 = _mm256_packs_epi32(x1, x2);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += dststride;
  }
}

static void
oh_hevc_put_hevc_bi0_qpel_v16_14_avx2(int16_t* dst,
                                    const uint16_t* _src,
                                    ptrdiff_t _srcstride,
                                    int height,
                                    intptr_t mx,
                                    intptr_t my,
                                    int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, x5, x6, x7, x8, x9, c1, c2, c3, c4;
  const uint16_t* src = _src;
  const int srcstride = _srcstride;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][3]);
  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * srcstride]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      x5 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x6 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x7 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      x8 = _mm256_loadu_si256((__m256i*)&src[x + 4 * srcstride]);
      x9 = x1;
      x1 = _mm256_unpacklo_epi16(x9, x2);
      x2 = _mm256_unpackhi_epi16(x9, x2);
      x9 = x3;
      x3 = _mm256_unpacklo_epi16(x9, x4);
      x4 = _mm256_unpackhi_epi16(x9, x4);
      x9 = x5;
      x5 = _mm256_unpacklo_epi16(x9, x6);
      x6 = _mm256_unpackhi_epi16(x9, x6);
      x9 = x7;
      x7 = _mm256_unpacklo_epi16(x9, x8);
      x8 = _mm256_unpackhi_epi16(x9, x8);
      x1 = _mm256_madd_epi16(x1, c1);
      x3 = _mm256_madd_epi16(x3, c2);
      x5 = _mm256_madd_epi16(x5, c3);
      x7 = _mm256_madd_epi16(x7, c4);
      x2 = _mm256_madd_epi16(x2, c1);
      x4 = _mm256_madd_epi16(x4, c2);
      x6 = _mm256_madd_epi16(x6, c3);
      x8 = _mm256_madd_epi16(x8, c4);
      x1 = _mm256_add_epi32(x1, x3);
      x3 = _mm256_add_epi32(x5, x7);
      x2 = _mm256_add_epi32(x2, x4);
      x4 = _mm256_add_epi32(x6, x8);
      x1 = _mm256_add_epi32(x1, x3);
      x2 = _mm256_add_epi32(x2, x4);
      x1 = _mm256_srai_epi32(x1, 6);
      x2 = _mm256_srai_epi32(x2, 6);
      x1 = _mm256_packs_epi32(x1, x2);
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += 128;
  }
}

static void
oh_hevc_put_hevc_bi1_qpel_v16_14_10_avx2(uint16_t* _dst,
                                       ptrdiff_t _dststride,
                                       const int16_t* _src,
                                       ptrdiff_t _srcstride,
                                       const int16_t* src2,
                                       int height,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, x5, x6, x7, x8, x9, c1, c2, c3, c4;
  const int16_t* src = _src;
  const int srcstride = _srcstride;
  const __m256i offset = _mm256_set1_epi16(1 << 10);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][3]);
  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      __m256i r5;
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * srcstride]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      x5 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x6 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x7 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      x8 = _mm256_loadu_si256((__m256i*)&src[x + 4 * srcstride]);
      x9 = x1;
      x1 = _mm256_unpacklo_epi16(x9, x2);
      x2 = _mm256_unpackhi_epi16(x9, x2);
      x9 = x3;
      x3 = _mm256_unpacklo_epi16(x9, x4);
      x4 = _mm256_unpackhi_epi16(x9, x4);
      x9 = x5;
      x5 = _mm256_unpacklo_epi16(x9, x6);
      x6 = _mm256_unpackhi_epi16(x9, x6);
      x9 = x7;
      x7 = _mm256_unpacklo_epi16(x9, x8);
      x8 = _mm256_unpackhi_epi16(x9, x8);
      x1 = _mm256_madd_epi16(x1, c1);
      x3 = _mm256_madd_epi16(x3, c2);
      x5 = _mm256_madd_epi16(x5, c3);
      x7 = _mm256_madd_epi16(x7, c4);
      x2 = _mm256_madd_epi16(x2, c1);
      x4 = _mm256_madd_epi16(x4, c2);
      x6 = _mm256_madd_epi16(x6, c3);
      x8 = _mm256_madd_epi16(x8, c4);
      x1 = _mm256_add_epi32(x1, x3);
      x3 = _mm256_add_epi32(x5, x7);
      x2 = _mm256_add_epi32(x2, x4);
      x4 = _mm256_add_epi32(x6, x8);
      x1 = _mm256_add_epi32(x1, x3);
      x2 = _mm256_add_epi32(x2, x4);
      x1 = _mm256_srai_epi32(x1, 6);
      x2 = _mm256_srai_epi32(x2, 6);
      x1 = _mm256_packs_epi32(x1, x2);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      x1 = _mm256_adds_epi16(x1, r5);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    src2 += 128;
    dst += dststride;
  }
}

static void
oh_hevc_put_hevc_uni_qpel_v16_14_10_avx2(uint16_t* _dst,
                                       ptrdiff_t _dststride,
                                       const int16_t* _src,
                                       ptrdiff_t _srcstride,
                                       int height,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, x5, x6, x7, x8, x9, c1, c2, c3, c4;
  const int16_t* src = _src;
  const int srcstride = _srcstride;
  const __m256i offset = _mm256_set1_epi16(1 << (10 + 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][3]);
  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * srcstride]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      x5 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x6 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x7 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      x8 = _mm256_loadu_si256((__m256i*)&src[x + 4 * srcstride]);
      x9 = x1;
      x1 = _mm256_unpacklo_epi16(x9, x2);
      x2 = _mm256_unpackhi_epi16(x9, x2);
      x9 = x3;
      x3 = _mm256_unpacklo_epi16(x9, x4);
      x4 = _mm256_unpackhi_epi16(x9, x4);
      x9 = x5;
      x5 = _mm256_unpacklo_epi16(x9, x6);
      x6 = _mm256_unpackhi_epi16(x9, x6);
      x9 = x7;
      x7 = _mm256_unpacklo_epi16(x9, x8);
      x8 = _mm256_unpackhi_epi16(x9, x8);
      x1 = _mm256_madd_epi16(x1, c1);
      x3 = _mm256_madd_epi16(x3, c2);
      x5 = _mm256_madd_epi16(x5, c3);
      x7 = _mm256_madd_epi16(x7, c4);
      x2 = _mm256_madd_epi16(x2, c1);
      x4 = _mm256_madd_epi16(x4, c2);
      x6 = _mm256_madd_epi16(x6, c3);
      x8 = _mm256_madd_epi16(x8, c4);
      x1 = _mm256_add_epi32(x1, x3);
      x3 = _mm256_add_epi32(x5, x7);
      x2 = _mm256_add_epi32(x2, x4);
      x4 = _mm256_add_epi32(x6, x8);
      x1 = _mm256_add_epi32(x1, x3);
      x2 = _mm256_add_epi32(x2, x4);
      x1 = _mm256_srai_epi32(x1, 6);
      x2 = _mm256_srai_epi32(x2, 6);
      x1 = _mm256_packs_epi32(x1, x2);
      x1 = _mm256_mulhrs_epi16(x1, offset);
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += dststride;
  }
}

static void
oh_hevc_put_hevc_bi0_qpel_hv16_10_avx2(int16_t* dst,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  int16_t tmp_array[(128 + 7) * 128];
  int16_t* tmp = tmp_array;
  const uint16_t* src = _src;
  const int srcstride = _srcstride;
  src -= 3 * srcstride;
  oh_hevc_put_hevc_bi0_qpel_h16_10_avx2(
    tmp, src, _srcstride, height + 7, mx, my, width);
  tmp = tmp_array + 3 * 128;
  oh_hevc_put_hevc_bi0_qpel_v16_14_avx2(
    dst, (const uint16_t*)tmp, 128, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_qpel_hv16_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     const int16_t* src2,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  int16_t tmp_array[(128 + 7) * 128];
  int16_t* tmp = tmp_array;
  const uint16_t* src = _src;
  const int srcstride = _srcstride;
  src -= 3 * srcstride;
  oh_hevc_put_hevc_bi0_qpel_h16_10_avx2(
    tmp, src, _srcstride, height + 7, mx, my, width);
  tmp = tmp_array + 3 * 128;
  oh_hevc_put_hevc_bi1_qpel_v16_14_10_avx2(
    dst, dststride, tmp, 128, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_qpel_hv16_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  int16_t tmp_array[(128 + 7) * 128];
  int16_t* tmp = tmp_array;
  const uint16_t* src = _src;
  const int srcstride = _srcstride;
  src -= 3 * srcstride;
  oh_hevc_put_hevc_bi0_qpel_h16_10_avx2(
    tmp, src, _srcstride, height + 7, mx, my, width);
  tmp = tmp_array + 3 * 128;
  oh_hevc_put_hevc_uni_qpel_v16_14_10_avx2(
    dst, dststride, tmp, 128, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_pel_pixels32_10_avx2(int16_t* dst,
                                         const uint16_t* _src,
                                         ptrdiff_t _srcstride,
                                         int height,
                                         intptr_t mx,
                                         intptr_t my,
                                         int width)
{
  oh_hevc_put_hevc_bi0_pel_pixels16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_pel_pixels32_10_avx2(uint16_t* dst,
                                         ptrdiff_t dststride,
                                         const uint16_t* _src,
                                         ptrdiff_t _srcstride,
                                         const int16_t* src2,
                                         int height,
                                         intptr_t mx,
                                         intptr_t my,
                                         int width)
{
  oh_hevc_put_hevc_bi1_pel_pixels16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_pel_pixels64_10_avx2(int16_t* dst,
                                         const uint16_t* _src,
                                         ptrdiff_t _srcstride,
                                         int height,
                                         intptr_t mx,
                                         intptr_t my,
                                         int width)
{
  oh_hevc_put_hevc_bi0_pel_pixels16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_pel_pixels64_10_avx2(uint16_t* dst,
                                         ptrdiff_t dststride,
                                         const uint16_t* _src,
                                         ptrdiff_t _srcstride,
                                         const int16_t* src2,
                                         int height,
                                         intptr_t mx,
                                         intptr_t my,
                                         int width)
{
  oh_hevc_put_hevc_bi1_pel_pixels16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}


static void
oh_hevc_put_hevc_bi0_qpel_h32_10_avx2(int16_t* dst,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi0_qpel_h16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_qpel_h32_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     const int16_t* src2,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi1_qpel_h16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_qpel_h32_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_uni_qpel_h16_10_avx2(
    dst, dststride, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_qpel_h64_10_avx2(int16_t* dst,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi0_qpel_h16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_qpel_h64_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     const int16_t* src2,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi1_qpel_h16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_qpel_h64_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_uni_qpel_h16_10_avx2(
    dst, dststride, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_qpel_v32_10_avx2(int16_t* dst,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi0_qpel_v16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_qpel_v32_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     const int16_t* src2,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi1_qpel_v16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_qpel_v32_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_uni_qpel_v16_10_avx2(
    dst, dststride, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_qpel_v64_10_avx2(int16_t* dst,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi0_qpel_v16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_qpel_v64_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     const int16_t* src2,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi1_qpel_v16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_qpel_v64_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_uni_qpel_v16_10_avx2(
    dst, dststride, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_qpel_hv32_10_avx2(int16_t* dst,
                                      const uint16_t* _src,
                                      ptrdiff_t _srcstride,
                                      int height,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  oh_hevc_put_hevc_bi0_qpel_hv16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_qpel_hv32_10_avx2(uint16_t* dst,
                                      ptrdiff_t dststride,
                                      const uint16_t* _src,
                                      ptrdiff_t _srcstride,
                                      const int16_t* src2,
                                      int height,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  oh_hevc_put_hevc_bi1_qpel_hv16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_qpel_hv32_10_avx2(uint16_t* dst,
                                      ptrdiff_t dststride,
                                      const uint16_t* _src,
                                      ptrdiff_t _srcstride,
                                      int height,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  oh_hevc_put_hevc_uni_qpel_hv16_10_avx2(
    dst, dststride, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_qpel_hv64_10_avx2(int16_t* dst,
                                      const uint16_t* _src,
                                      ptrdiff_t _srcstride,
                                      int height,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  oh_hevc_put_hevc_bi0_qpel_hv16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_qpel_hv64_10_avx2(uint16_t* dst,
                                      ptrdiff_t dststride,
                                      const uint16_t* _src,
                                      ptrdiff_t _srcstride,
                                      const int16_t* src2,
                                      int height,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  oh_hevc_put_hevc_bi1_qpel_hv16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_qpel_hv64_10_avx2(uint16_t* dst,
                                      ptrdiff_t dststride,
                                      const uint16_t* _src,
                                      ptrdiff_t _srcstride,
                                      int height,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  oh_hevc_put_hevc_uni_qpel_hv16_10_avx2(
    dst, dststride, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_epel_h32_10_avx2(int16_t* dst,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi0_epel_h16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_epel_h32_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     const int16_t* src2,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi1_epel_h16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_epel_h32_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_uni_epel_h16_10_avx2(
    dst, dststride, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_epel_h64_10_avx2(int16_t* dst,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi0_epel_h16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_epel_h64_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     const int16_t* src2,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi1_epel_h16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_epel_h64_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_uni_epel_h16_10_avx2(
    dst, dststride, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_epel_v32_10_avx2(int16_t* dst,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi0_epel_v16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_epel_v32_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     const int16_t* src2,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi1_epel_v16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_epel_v32_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_uni_epel_v16_10_avx2(
    dst, dststride, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_epel_v64_10_avx2(int16_t* dst,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi0_epel_v16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_epel_v64_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     const int16_t* src2,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_bi1_epel_v16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_epel_v64_10_avx2(uint16_t* dst,
                                     ptrdiff_t dststride,
                                     const uint16_t* _src,
                                     ptrdiff_t _srcstride,
                                     int height,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  oh_hevc_put_hevc_uni_epel_v16_10_avx2(
    dst, dststride, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_epel_hv32_10_avx2(int16_t* dst,
                                      const uint16_t* _src,
                                      ptrdiff_t _srcstride,
                                      int height,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  oh_hevc_put_hevc_bi0_epel_hv16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_epel_hv32_10_avx2(uint16_t* dst,
                                      ptrdiff_t dststride,
                                      const uint16_t* _src,
                                      ptrdiff_t _srcstride,
                                      const int16_t* src2,
                                      int height,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  oh_hevc_put_hevc_bi1_epel_hv16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_epel_hv32_10_avx2(uint16_t* dst,
                                      ptrdiff_t dststride,
                                      const uint16_t* _src,
                                      ptrdiff_t _srcstride,
                                      int height,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  oh_hevc_put_hevc_uni_epel_hv16_10_avx2(
    dst, dststride, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi0_epel_hv64_10_avx2(int16_t* dst,
                                      const uint16_t* _src,
                                      ptrdiff_t _srcstride,
                                      int height,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  oh_hevc_put_hevc_bi0_epel_hv16_10_avx2(
    dst, _src, _srcstride, height, mx, my, width);
}

static void
oh_hevc_put_hevc_bi1_epel_hv64_10_avx2(uint16_t* dst,
                                      ptrdiff_t dststride,
                                      const uint16_t* _src,
                                      ptrdiff_t _srcstride,
                                      const int16_t* src2,
                                      int height,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  oh_hevc_put_hevc_bi1_epel_hv16_10_avx2(
    dst, dststride, _src, _srcstride, src2, height, mx, my, width);
}

static void
oh_hevc_put_hevc_uni_epel_hv64_10_avx2(uint16_t* dst,
                                      ptrdiff_t dststride,
                                      const uint16_t* _src,
                                      ptrdiff_t _srcstride,
                                      int height,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  oh_hevc_put_hevc_uni_epel_hv16_10_avx2(
    dst, dststride, _src, _srcstride, height, mx, my, width);
}

static void
put_vvc_qpel_h16_10_avx2(int16_t* dst,
                                ptrdiff_t dststride,
                                uint8_t* _src,
                                ptrdiff_t _srcstride,
                                int height,
                                intptr_t mx,
                                intptr_t my,
                                int width)
{
  int x, y;
  int shift = 10 - 8;
  __m256i x1, x2, x3, x4, r1, r2, r3, r4, c1, c2, c3, c4, t1, t2;
  uint16_t* src = (uint16_t*)_src;
  const int srcstride = _srcstride >> 1;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][3]);
  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * 1]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      t1 = _mm256_madd_epi16(t1, c1);
      t2 = _mm256_madd_epi16(t2, c2);
      r2 = _mm256_add_epi32(t1, t2);
      x1 = _mm256_madd_epi16(x1, c1);
      x2 = _mm256_madd_epi16(x2, c2);
      r1 = _mm256_add_epi32(x1, x2);
      x1 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 4 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      t1 = _mm256_madd_epi16(t1, c3);
      t2 = _mm256_madd_epi16(t2, c4);
      r4 = _mm256_add_epi32(t1, t2);
      x1 = _mm256_madd_epi16(x1, c3);
      x2 = _mm256_madd_epi16(x2, c4);
      r3 = _mm256_add_epi32(x1, x2);
      x1 = _mm256_add_epi32(r1, r3);
      x2 = _mm256_add_epi32(r2, r4);
      x1 = _mm256_srai_epi32(x1, shift);
      x2 = _mm256_srai_epi32(x2, shift);
      x1 = _mm256_packs_epi32(x1, x2);
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += dststride;
  }
}

static void
put_vvc_bi_w_qpel_v16_14_10_avx2(uint8_t* _dst,
                                        ptrdiff_t _dststride,
                                        uint8_t* _src,
                                        ptrdiff_t _srcstride,
                                        int16_t* src2,
                                        ptrdiff_t src2stride,
                                        int height,
                                        int denom,
                                        int _wx0,
                                        int _wx1,

                                        intptr_t mx,
                                        intptr_t my,
                                        int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, x5, x6, x7, x8, x9, c1, c2, c3, c4;
  uint16_t* src = (uint16_t*)_src;
  const int srcstride = _srcstride >> 1;
  const int log2Wd = denom + 14 - 10 - 1;
  const int shift2 = log2Wd + 1;


  const __m256i wx0 = _mm256_set1_epi16(_wx0);
  const __m256i wx1 = _mm256_set1_epi16(_wx1);
  const __m256i offset = _mm256_set1_epi32(1 << log2Wd);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][3]);
  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      __m256i r5;
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * srcstride]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      x5 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x6 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x7 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      x8 = _mm256_loadu_si256((__m256i*)&src[x + 4 * srcstride]);
      x9 = x1;
      x1 = _mm256_unpacklo_epi16(x9, x2);
      x2 = _mm256_unpackhi_epi16(x9, x2);
      x9 = x3;
      x3 = _mm256_unpacklo_epi16(x9, x4);
      x4 = _mm256_unpackhi_epi16(x9, x4);
      x9 = x5;
      x5 = _mm256_unpacklo_epi16(x9, x6);
      x6 = _mm256_unpackhi_epi16(x9, x6);
      x9 = x7;
      x7 = _mm256_unpacklo_epi16(x9, x8);
      x8 = _mm256_unpackhi_epi16(x9, x8);
      x1 = _mm256_madd_epi16(x1, c1);
      x3 = _mm256_madd_epi16(x3, c2);
      x5 = _mm256_madd_epi16(x5, c3);
      x7 = _mm256_madd_epi16(x7, c4);
      x2 = _mm256_madd_epi16(x2, c1);
      x4 = _mm256_madd_epi16(x4, c2);
      x6 = _mm256_madd_epi16(x6, c3);
      x8 = _mm256_madd_epi16(x8, c4);
      x1 = _mm256_add_epi32(x1, x3);
      x3 = _mm256_add_epi32(x5, x7);
      x2 = _mm256_add_epi32(x2, x4);
      x4 = _mm256_add_epi32(x6, x8);
      x1 = _mm256_add_epi32(x1, x3);
      x2 = _mm256_add_epi32(x2, x4);
      x1 = _mm256_srai_epi32(x1, 6);
      x2 = _mm256_srai_epi32(x2, 6);
      x1 = _mm256_packs_epi32(x1, x2);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      {
        __m256i x3, x4, r7, r8;
        x3 = _mm256_mulhi_epi16(x1, wx1);
        x1 = _mm256_mullo_epi16(x1, wx1);
        r7 = _mm256_mulhi_epi16(r5, wx0);
        r5 = _mm256_mullo_epi16(r5, wx0);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        r8 = _mm256_unpackhi_epi16(r5, r7);
        r5 = _mm256_unpacklo_epi16(r5, r7);
        x4 = _mm256_add_epi32(x4, r8);
        x1 = _mm256_add_epi32(x1, r5);
        x4 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x1 = _mm256_packus_epi32(x1, x4);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    src2 += src2stride;
    dst += dststride;
  }
}

static void
put_vvc_uni_w_qpel_v16_14_10_avx2(uint8_t* _dst,
                                         ptrdiff_t _dststride,
                                         uint8_t* _src,
                                         ptrdiff_t _srcstride,
                                         int height,
                                         int denom,
                                         int _wx,
                                         int _ox,
                                         intptr_t mx,
                                         intptr_t my,
                                         int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, x5, x6, x7, x8, x9, c1, c2, c3, c4;
  uint16_t* src = (uint16_t*)_src;
  const int srcstride = _srcstride >> 1;
  const int shift2 = denom + 14 - 10;
  const __m256i ox = _mm256_set1_epi32(_ox << (10 - 8));
  const __m256i wx = _mm256_set1_epi16(_wx);
  const __m256i offset = _mm256_set1_epi32(1 << (shift2 - 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][3]);
  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * srcstride]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      x5 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x6 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x7 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      x8 = _mm256_loadu_si256((__m256i*)&src[x + 4 * srcstride]);
      x9 = x1;
      x1 = _mm256_unpacklo_epi16(x9, x2);
      x2 = _mm256_unpackhi_epi16(x9, x2);
      x9 = x3;
      x3 = _mm256_unpacklo_epi16(x9, x4);
      x4 = _mm256_unpackhi_epi16(x9, x4);
      x9 = x5;
      x5 = _mm256_unpacklo_epi16(x9, x6);
      x6 = _mm256_unpackhi_epi16(x9, x6);
      x9 = x7;
      x7 = _mm256_unpacklo_epi16(x9, x8);
      x8 = _mm256_unpackhi_epi16(x9, x8);
      x1 = _mm256_madd_epi16(x1, c1);
      x3 = _mm256_madd_epi16(x3, c2);
      x5 = _mm256_madd_epi16(x5, c3);
      x7 = _mm256_madd_epi16(x7, c4);
      x2 = _mm256_madd_epi16(x2, c1);
      x4 = _mm256_madd_epi16(x4, c2);
      x6 = _mm256_madd_epi16(x6, c3);
      x8 = _mm256_madd_epi16(x8, c4);
      x1 = _mm256_add_epi32(x1, x3);
      x3 = _mm256_add_epi32(x5, x7);
      x2 = _mm256_add_epi32(x2, x4);
      x4 = _mm256_add_epi32(x6, x8);
      x1 = _mm256_add_epi32(x1, x3);
      x2 = _mm256_add_epi32(x2, x4);
      x1 = _mm256_srai_epi32(x1, 6);
      x2 = _mm256_srai_epi32(x2, 6);
      x1 = _mm256_packs_epi32(x1, x2);
      {
        __m256i x3, x4;
        x3 = _mm256_mulhi_epi16(x1, wx);
        x1 = _mm256_mullo_epi16(x1, wx);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        x3 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x3 = _mm256_add_epi32(x3, ox);
        x1 = _mm256_add_epi32(x1, ox);
        x1 = _mm256_packus_epi32(x1, x3);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += dststride;
  }
}

static void
put_vvc_uni_w_pel_pixels8_10_avx2(uint8_t* _dst,
                                          ptrdiff_t _dststride,
                                          uint8_t* _src,
                                          ptrdiff_t _srcstride,
                                          int height,
                                          int denom,
                                          int _wx,
                                          int _ox,
                                          intptr_t mx,
                                          intptr_t my,
                                          int width)
{
  int x, y;
  __m256i x1;
  uint16_t* src = (uint16_t*)_src;
  const int srcstride = _srcstride >> 1;
  const int shift2 = denom + 14 - 10;
  const __m256i ox = _mm256_set1_epi32(_ox << (10 - 8));
  const __m256i wx = _mm256_set1_epi16(_wx);
  const __m256i offset = _mm256_set1_epi32(1 << (shift2 - 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x1 = _mm256_slli_epi16(x1, 14 - 10);
      {
        __m256i x3, x4;
        x3 = _mm256_mulhi_epi16(x1, wx);
        x1 = _mm256_mullo_epi16(x1, wx);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        x3 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x3 = _mm256_add_epi32(x3, ox);
        x1 = _mm256_add_epi32(x1, ox);
        x1 = _mm256_packus_epi32(x1, x3);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += dststride;
  }
}
static void
put_vvc_bi_w_pel_pixels8_10_avx2(uint8_t* _dst,
                                         ptrdiff_t _dststride,
                                         uint8_t* _src,
                                         ptrdiff_t _srcstride,
                                         int16_t* src2,
                                         ptrdiff_t src2stride,
                                         int height,
                                         int denom,
                                         int _wx0,
                                         int _wx1,
                                         intptr_t mx,
                                         intptr_t my,
                                         int width)
{
  int x, y;
  __m256i x1;
  uint16_t* src = (uint16_t*)_src;
  const int srcstride = _srcstride >> 1;
  const int log2Wd = denom + 14 - 10 - 1;
  const int shift2 = log2Wd + 1;
  const __m256i wx0 = _mm256_set1_epi16(_wx0);
  const __m256i wx1 = _mm256_set1_epi16(_wx1);
  const __m256i offset = _mm256_set1_epi32(1 << log2Wd);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      __m256i r5;
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x1 = _mm256_slli_epi16(x1, 14 - 10);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      {
        __m256i x3, x4, r7, r8;
        x3 = _mm256_mulhi_epi16(x1, wx1);
        x1 = _mm256_mullo_epi16(x1, wx1);
        r7 = _mm256_mulhi_epi16(r5, wx0);
        r5 = _mm256_mullo_epi16(r5, wx0);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        r8 = _mm256_unpackhi_epi16(r5, r7);
        r5 = _mm256_unpacklo_epi16(r5, r7);
        x4 = _mm256_add_epi32(x4, r8);
        x1 = _mm256_add_epi32(x1, r5);
        x4 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x1 = _mm256_packus_epi32(x1, x4);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    src2 += src2stride;
    dst += dststride;
  }
}

static void
put_vvc_uni_w_epel_h16_10_avx2(uint8_t* _dst,
                                      ptrdiff_t _dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int height,
                                      int denom,
                                      int _wx,
                                      int _ox,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2;
  uint16_t* src = ((uint16_t*)_src) - 1;
  ptrdiff_t srcstride = _srcstride >> 1;
  const int shift2 = denom + 14 - 10;
  const __m256i ox = _mm256_set1_epi32(_ox << (10 - 8));
  const __m256i wx = _mm256_set1_epi16(_wx);
  const __m256i offset = _mm256_set1_epi32(1 << (shift2 - 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][1]);
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      x1 = _mm256_packs_epi32(x1, t1);
      {
        __m256i x3, x4;
        x3 = _mm256_mulhi_epi16(x1, wx);
        x1 = _mm256_mullo_epi16(x1, wx);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        x3 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x3 = _mm256_add_epi32(x3, ox);
        x1 = _mm256_add_epi32(x1, ox);
        x1 = _mm256_packus_epi32(x1, x3);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += dststride;
  }
}
static void
put_vvc_bi_w_epel_h16_10_avx2(uint8_t* _dst,
                                     ptrdiff_t _dststride,
                                     uint8_t* _src,
                                     ptrdiff_t _srcstride,
                                     int16_t* src2,
                                     ptrdiff_t src2stride,
                                     int height,
                                     int denom,
                                     int _wx0,
                                     int _wx1,
                                                                          intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2;
  uint16_t* src = ((uint16_t*)_src) - 1;
  ptrdiff_t srcstride = _srcstride >> 1;
  const int log2Wd = denom + 14 - 10 - 1;
  const int shift2 = log2Wd + 1;


  const __m256i wx0 = _mm256_set1_epi16(_wx0);
  const __m256i wx1 = _mm256_set1_epi16(_wx1);
  const __m256i offset = _mm256_set1_epi32(1 << log2Wd);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][1]);
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      __m256i r5;
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      x1 = _mm256_packs_epi32(x1, t1);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      {
        __m256i x3, x4, r7, r8;
        x3 = _mm256_mulhi_epi16(x1, wx1);
        x1 = _mm256_mullo_epi16(x1, wx1);
        r7 = _mm256_mulhi_epi16(r5, wx0);
        r5 = _mm256_mullo_epi16(r5, wx0);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        r8 = _mm256_unpackhi_epi16(r5, r7);
        r5 = _mm256_unpacklo_epi16(r5, r7);
        x4 = _mm256_add_epi32(x4, r8);
        x1 = _mm256_add_epi32(x1, r5);
        x4 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x1 = _mm256_packus_epi32(x1, x4);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    src2 += src2stride;
    dst += dststride;
  }
}

static void
put_vvc_uni_w_epel_v16_10_avx2(uint8_t* _dst,
                                      ptrdiff_t _dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int height,
                                      int denom,
                                      int _wx,
                                      int _ox,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2;
  ptrdiff_t srcstride = _srcstride >> 1;
  uint16_t* src = ((uint16_t*)_src) - srcstride;
  const int shift2 = denom + 14 - 10;
  const __m256i ox = _mm256_set1_epi32(_ox << (10 - 8));
  const __m256i wx = _mm256_set1_epi16(_wx);
  const __m256i offset = _mm256_set1_epi32(1 << (shift2 - 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][1]);
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      x1 = _mm256_packs_epi32(x1, t2);
      {
        __m256i x3, x4;
        x3 = _mm256_mulhi_epi16(x1, wx);
        x1 = _mm256_mullo_epi16(x1, wx);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        x3 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x3 = _mm256_add_epi32(x3, ox);
        x1 = _mm256_add_epi32(x1, ox);
        x1 = _mm256_packus_epi32(x1, x3);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += dststride;
  }
}
static void
put_vvc_bi_w_epel_v16_10_avx2(uint8_t* _dst,
                                     ptrdiff_t _dststride,
                                     uint8_t* _src,
                                     ptrdiff_t _srcstride,
                                     int16_t* src2,
                                     ptrdiff_t src2stride,
                                     int height,
                                     int denom,
                                     int _wx0,
                                     int _wx1,
                                                                          intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2;
  ptrdiff_t srcstride = _srcstride >> 1;
  uint16_t* src = ((uint16_t*)_src) - srcstride;
  const int log2Wd = denom + 14 - 10 - 1;
  const int shift2 = log2Wd + 1;


  const __m256i wx0 = _mm256_set1_epi16(_wx0);
  const __m256i wx1 = _mm256_set1_epi16(_wx1);
  const __m256i offset = _mm256_set1_epi32(1 << log2Wd);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][1]);
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      __m256i r5;
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      x1 = _mm256_packs_epi32(x1, t1);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      {
        __m256i x3, x4, r7, r8;
        x3 = _mm256_mulhi_epi16(x1, wx1);
        x1 = _mm256_mullo_epi16(x1, wx1);
        r7 = _mm256_mulhi_epi16(r5, wx0);
        r5 = _mm256_mullo_epi16(r5, wx0);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        r8 = _mm256_unpackhi_epi16(r5, r7);
        r5 = _mm256_unpacklo_epi16(r5, r7);
        x4 = _mm256_add_epi32(x4, r8);
        x1 = _mm256_add_epi32(x1, r5);
        x4 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x1 = _mm256_packus_epi32(x1, x4);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    src2 += src2stride;
    dst += dststride;
  }
}

static void
put_vvc_uni_w_epel_hv16_10_avx2(uint8_t* _dst,
                                       ptrdiff_t _dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int height,
                                       int denom,
                                       int _wx,
                                       int _ox,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2, f3, f4, r1, r2, r3, r4;
  const int shift2 = denom + 14 - 10;
  const __m256i ox = _mm256_set1_epi32(_ox << (10 - 8));
  const __m256i wx = _mm256_set1_epi16(_wx);
  const __m256i offset = _mm256_set1_epi32(1 << (shift2 - 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  uint16_t* dst_bis = dst;
  uint16_t *src_bis, *src = ((uint16_t*)_src) - 1;
  ptrdiff_t srcstride = _srcstride >> 1;
  src -= EPEL_EXTRA_BEFORE * srcstride;
  src_bis = src;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][1]);
  f3 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][0]);
  f4 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][1]);
  for (x = 0; x < width; x += 16) {
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r1 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r2 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r3 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    for (y = 0; y < height; y++) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      r4 = _mm256_packs_epi32(x1, t1);
      src += srcstride;
      t1 = _mm256_unpackhi_epi16(r1, r2);
      x1 = _mm256_unpacklo_epi16(r1, r2);
      t2 = _mm256_unpackhi_epi16(r3, r4);
      x2 = _mm256_unpacklo_epi16(r3, r4);
      x1 = _mm256_madd_epi16(x1, f3);
      t1 = _mm256_madd_epi16(t1, f3);
      x2 = _mm256_madd_epi16(x2, f4);
      t2 = _mm256_madd_epi16(t2, f4);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 6);
      x1 = _mm256_srai_epi32(x1, 6);
      x1 = _mm256_packs_epi32(x1, t1);
      {
        __m256i x3, x4;
        x3 = _mm256_mulhi_epi16(x1, wx);
        x1 = _mm256_mullo_epi16(x1, wx);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        x3 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x3 = _mm256_add_epi32(x3, ox);
        x1 = _mm256_add_epi32(x1, ox);
        x1 = _mm256_packus_epi32(x1, x3);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
      dst += dststride;
      r1 = r2;
      r2 = r3;
      r3 = r4;
    }
    src = src_bis;
    dst = dst_bis;
  }
}
static void
put_vvc_bi_w_epel_hv16_10_avx2(uint8_t* _dst,
                                      ptrdiff_t _dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int16_t* src2,
                                      ptrdiff_t src2stride,
                                      int height,
                                      int denom,
                                      int _wx0,
                                      int _wx1,

                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, t1, t2, f1, f2, f3, f4, r1, r2, r3, r4;
  const int log2Wd = denom + 14 - 10 - 1;
  const int shift2 = log2Wd + 1;


  const __m256i wx0 = _mm256_set1_epi16(_wx0);
  const __m256i wx1 = _mm256_set1_epi16(_wx1);
  const __m256i offset = _mm256_set1_epi32(1 << log2Wd);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  int16_t* src2_bis = src2;
  uint16_t* dst_bis = dst;
  uint16_t *src_bis, *src = ((uint16_t*)_src) - 1;
  ptrdiff_t srcstride = _srcstride >> 1;
  src -= EPEL_EXTRA_BEFORE * srcstride;
  src_bis = src;
  f1 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][0]);
  f2 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[mx - 1][1]);
  f3 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][0]);
  f4 = _mm256_load_si256((__m256i*)oh_hevc_epel_filters_avx2[my - 1][1]);
  for (x = 0; x < width; x += 16) {
    __m256i r5;
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r1 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r2 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    x1 = _mm256_loadu_si256((__m256i*)&src[x]);
    x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
    x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
    x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
    t1 = _mm256_unpackhi_epi16(x1, x2);
    x1 = _mm256_unpacklo_epi16(x1, x2);
    t2 = _mm256_unpackhi_epi16(x3, x4);
    x2 = _mm256_unpacklo_epi16(x3, x4);
    x1 = _mm256_madd_epi16(x1, f1);
    t1 = _mm256_madd_epi16(t1, f1);
    x2 = _mm256_madd_epi16(x2, f2);
    t2 = _mm256_madd_epi16(t2, f2);
    x1 = _mm256_add_epi32(x1, x2);
    t1 = _mm256_add_epi32(t1, t2);
    t1 = _mm256_srai_epi32(t1, 2);
    x1 = _mm256_srai_epi32(x1, 2);
    r3 = _mm256_packs_epi32(x1, t1);
    src += srcstride;
    for (y = 0; y < height; y++) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      x1 = _mm256_madd_epi16(x1, f1);
      t1 = _mm256_madd_epi16(t1, f1);
      x2 = _mm256_madd_epi16(x2, f2);
      t2 = _mm256_madd_epi16(t2, f2);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 2);
      x1 = _mm256_srai_epi32(x1, 2);
      r4 = _mm256_packs_epi32(x1, t1);
      src += srcstride;
      t1 = _mm256_unpackhi_epi16(r1, r2);
      x1 = _mm256_unpacklo_epi16(r1, r2);
      t2 = _mm256_unpackhi_epi16(r3, r4);
      x2 = _mm256_unpacklo_epi16(r3, r4);
      x1 = _mm256_madd_epi16(x1, f3);
      t1 = _mm256_madd_epi16(t1, f3);
      x2 = _mm256_madd_epi16(x2, f4);
      t2 = _mm256_madd_epi16(t2, f4);
      x1 = _mm256_add_epi32(x1, x2);
      t1 = _mm256_add_epi32(t1, t2);
      t1 = _mm256_srai_epi32(t1, 6);
      x1 = _mm256_srai_epi32(x1, 6);
      x1 = _mm256_packs_epi32(x1, t1);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      {
        __m256i x3, x4, r7, r8;
        x3 = _mm256_mulhi_epi16(x1, wx1);
        x1 = _mm256_mullo_epi16(x1, wx1);
        r7 = _mm256_mulhi_epi16(r5, wx0);
        r5 = _mm256_mullo_epi16(r5, wx0);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        r8 = _mm256_unpackhi_epi16(r5, r7);
        r5 = _mm256_unpacklo_epi16(r5, r7);
        x4 = _mm256_add_epi32(x4, r8);
        x1 = _mm256_add_epi32(x1, r5);
        x4 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x1 = _mm256_packus_epi32(x1, x4);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
      src2 += src2stride;
      dst += dststride;
      r1 = r2;
      r2 = r3;
      r3 = r4;
    }
    src = src_bis;
    src2 = src2_bis;
    dst = dst_bis;
  }
}

static void
put_vvc_uni_w_qpel_h16_10_avx2(uint8_t* _dst,
                                      ptrdiff_t _dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int height,
                                      int denom,
                                      int _wx,
                                      int _ox,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  int x, y;
  int shift = 10 - 8;
  __m256i x1, x2, x3, x4, r1, r2, r3, r4, c1, c2, c3, c4, t1, t2;
  uint16_t* src = (uint16_t*)_src;
  const int srcstride = _srcstride >> 1;
  const int shift2 = denom + 14 - 10;
  const __m256i ox = _mm256_set1_epi32(_ox << (10 - 8));
  const __m256i wx = _mm256_set1_epi16(_wx);
  const __m256i offset = _mm256_set1_epi32(1 << (shift2 - 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][3]);
  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * 1]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      t1 = _mm256_madd_epi16(t1, c1);
      t2 = _mm256_madd_epi16(t2, c2);
      r2 = _mm256_add_epi32(t1, t2);
      x1 = _mm256_madd_epi16(x1, c1);
      x2 = _mm256_madd_epi16(x2, c2);
      r1 = _mm256_add_epi32(x1, x2);
      x1 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 4 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      t1 = _mm256_madd_epi16(t1, c3);
      t2 = _mm256_madd_epi16(t2, c4);
      r4 = _mm256_add_epi32(t1, t2);
      x1 = _mm256_madd_epi16(x1, c3);
      x2 = _mm256_madd_epi16(x2, c4);
      r3 = _mm256_add_epi32(x1, x2);
      x1 = _mm256_add_epi32(r1, r3);
      x2 = _mm256_add_epi32(r2, r4);
      x1 = _mm256_srai_epi32(x1, shift);
      x2 = _mm256_srai_epi32(x2, shift);
      x1 = _mm256_packs_epi32(x1, x2);
      {
        __m256i x3, x4;
        x3 = _mm256_mulhi_epi16(x1, wx);
        x1 = _mm256_mullo_epi16(x1, wx);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        x3 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x3 = _mm256_add_epi32(x3, ox);
        x1 = _mm256_add_epi32(x1, ox);
        x1 = _mm256_packus_epi32(x1, x3);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += dststride;
  }
}
static void
put_vvc_bi_w_qpel_h16_10_avx2(uint8_t* _dst,
                                     ptrdiff_t _dststride,
                                     uint8_t* _src,
                                     ptrdiff_t _srcstride,
                                     int16_t* src2,
                                     ptrdiff_t src2stride,
                                     int height,
                                     int denom,
                                     int _wx0,
                                     int _wx1,
                                                                          intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  int x, y;
  int shift = 10 - 8;
  __m256i x1, x2, x3, x4, r1, r2, r3, r4, c1, c2, c3, c4, t1, t2;
  uint16_t* src = (uint16_t*)_src;
  const int srcstride = _srcstride >> 1;
  const int log2Wd = denom + 14 - 10 - 1;
  const int shift2 = log2Wd + 1;


  const __m256i wx0 = _mm256_set1_epi16(_wx0);
  const __m256i wx1 = _mm256_set1_epi16(_wx1);
  const __m256i offset = _mm256_set1_epi32(1 << log2Wd);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[mx - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[mx - 1][3]);
  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      __m256i r5;
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * 1]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      t1 = _mm256_madd_epi16(t1, c1);
      t2 = _mm256_madd_epi16(t2, c2);
      r2 = _mm256_add_epi32(t1, t2);
      x1 = _mm256_madd_epi16(x1, c1);
      x2 = _mm256_madd_epi16(x2, c2);
      r1 = _mm256_add_epi32(x1, x2);
      x1 = _mm256_loadu_si256((__m256i*)&src[x + 1]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x + 2 * 1]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x + 3 * 1]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x + 4 * 1]);
      t1 = _mm256_unpackhi_epi16(x1, x2);
      x1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x3, x4);
      x2 = _mm256_unpacklo_epi16(x3, x4);
      t1 = _mm256_madd_epi16(t1, c3);
      t2 = _mm256_madd_epi16(t2, c4);
      r4 = _mm256_add_epi32(t1, t2);
      x1 = _mm256_madd_epi16(x1, c3);
      x2 = _mm256_madd_epi16(x2, c4);
      r3 = _mm256_add_epi32(x1, x2);
      x1 = _mm256_add_epi32(r1, r3);
      x2 = _mm256_add_epi32(r2, r4);
      x1 = _mm256_srai_epi32(x1, shift);
      x2 = _mm256_srai_epi32(x2, shift);
      x1 = _mm256_packs_epi32(x1, x2);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      {
        __m256i x3, x4, r7, r8;
        x3 = _mm256_mulhi_epi16(x1, wx1);
        x1 = _mm256_mullo_epi16(x1, wx1);
        r7 = _mm256_mulhi_epi16(r5, wx0);
        r5 = _mm256_mullo_epi16(r5, wx0);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        r8 = _mm256_unpackhi_epi16(r5, r7);
        r5 = _mm256_unpacklo_epi16(r5, r7);
        x4 = _mm256_add_epi32(x4, r8);
        x1 = _mm256_add_epi32(x1, r5);
        x4 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x1 = _mm256_packus_epi32(x1, x4);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    src2 += src2stride;
    dst += dststride;
  }
}

static void
put_vvc_uni_w_qpel_v16_10_avx2(uint8_t* _dst,
                                      ptrdiff_t _dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int height,
                                      int denom,
                                      int _wx,
                                      int _ox,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, x5, x6, x7, x8, x9, c1, c2, c3, c4;
  uint16_t* src = (uint16_t*)_src;
  const int srcstride = _srcstride >> 1;
  const int shift2 = denom + 14 - 10;
  const __m256i ox = _mm256_set1_epi32(_ox << (10 - 8));
  const __m256i wx = _mm256_set1_epi16(_wx);
  const __m256i offset = _mm256_set1_epi32(1 << (shift2 - 1));
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][3]);
  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * srcstride]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      x5 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x6 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x7 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      x8 = _mm256_loadu_si256((__m256i*)&src[x + 4 * srcstride]);
      x9 = x1;
      x1 = _mm256_unpacklo_epi16(x9, x2);
      x2 = _mm256_unpackhi_epi16(x9, x2);
      x9 = x3;
      x3 = _mm256_unpacklo_epi16(x9, x4);
      x4 = _mm256_unpackhi_epi16(x9, x4);
      x9 = x5;
      x5 = _mm256_unpacklo_epi16(x9, x6);
      x6 = _mm256_unpackhi_epi16(x9, x6);
      x9 = x7;
      x7 = _mm256_unpacklo_epi16(x9, x8);
      x8 = _mm256_unpackhi_epi16(x9, x8);
      x1 = _mm256_madd_epi16(x1, c1);
      x3 = _mm256_madd_epi16(x3, c2);
      x5 = _mm256_madd_epi16(x5, c3);
      x7 = _mm256_madd_epi16(x7, c4);
      x2 = _mm256_madd_epi16(x2, c1);
      x4 = _mm256_madd_epi16(x4, c2);
      x6 = _mm256_madd_epi16(x6, c3);
      x8 = _mm256_madd_epi16(x8, c4);
      x1 = _mm256_add_epi32(x1, x3);
      x3 = _mm256_add_epi32(x5, x7);
      x2 = _mm256_add_epi32(x2, x4);
      x4 = _mm256_add_epi32(x6, x8);
      x1 = _mm256_add_epi32(x1, x3);
      x2 = _mm256_add_epi32(x2, x4);
      x1 = _mm256_srai_epi32(x1, 2);
      x2 = _mm256_srai_epi32(x2, 2);
      x1 = _mm256_packs_epi32(x1, x2);
      {
        __m256i x3, x4;
        x3 = _mm256_mulhi_epi16(x1, wx);
        x1 = _mm256_mullo_epi16(x1, wx);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        x3 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x3 = _mm256_add_epi32(x3, ox);
        x1 = _mm256_add_epi32(x1, ox);
        x1 = _mm256_packus_epi32(x1, x3);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    dst += dststride;
  }
}
static void
put_vvc_bi_w_qpel_v16_10_avx2(uint8_t* _dst,
                                     ptrdiff_t _dststride,
                                     uint8_t* _src,
                                     ptrdiff_t _srcstride,
                                     int16_t* src2,
                                     ptrdiff_t src2stride,
                                     int height,
                                     int denom,
                                     int _wx0,
                                     int _wx1,
                                     intptr_t mx,
                                     intptr_t my,
                                     int width)
{
  int x, y;
  __m256i x1, x2, x3, x4, x5, x6, x7, x8, x9, c1, c2, c3, c4;
  uint16_t* src = (uint16_t*)_src;
  const int srcstride = _srcstride >> 1;
  const int log2Wd = denom + 14 - 10 - 1;
  const int shift2 = log2Wd + 1;
  const __m256i wx0 = _mm256_set1_epi16(_wx0);
  const __m256i wx1 = _mm256_set1_epi16(_wx1);
  const __m256i offset = _mm256_set1_epi32(1 << log2Wd);
  uint16_t* dst = (uint16_t*)_dst;
  const int dststride = _dststride >> 1;
  #if 0
  c1 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][0]);
  c2 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][1]);
  c3 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][2]);
  c4 = _mm256_load_si256((__m256i*)oh_hevc_qpel_filters_avx2[my - 1][3]);
  #else
  c1 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][0]);
  c2 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][1]);
  c3 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][2]);
  c4 = _mm256_set1_epi32(ov_mcp_filters_l[my - 1][3]);
  #endif
  for (y = 0; y < height; y++) {
    for (x = 0; x < width; x += 16) {
      __m256i r5;
      x1 = _mm256_loadu_si256((__m256i*)&src[x - 3 * srcstride]);
      x2 = _mm256_loadu_si256((__m256i*)&src[x - 2 * srcstride]);
      x3 = _mm256_loadu_si256((__m256i*)&src[x - 1 * srcstride]);
      x4 = _mm256_loadu_si256((__m256i*)&src[x]);
      x5 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);
      x6 = _mm256_loadu_si256((__m256i*)&src[x + 2 * srcstride]);
      x7 = _mm256_loadu_si256((__m256i*)&src[x + 3 * srcstride]);
      x8 = _mm256_loadu_si256((__m256i*)&src[x + 4 * srcstride]);
      x9 = x1;
      x1 = _mm256_unpacklo_epi16(x9, x2);
      x2 = _mm256_unpackhi_epi16(x9, x2);
      x9 = x3;
      x3 = _mm256_unpacklo_epi16(x9, x4);
      x4 = _mm256_unpackhi_epi16(x9, x4);
      x9 = x5;
      x5 = _mm256_unpacklo_epi16(x9, x6);
      x6 = _mm256_unpackhi_epi16(x9, x6);
      x9 = x7;
      x7 = _mm256_unpacklo_epi16(x9, x8);
      x8 = _mm256_unpackhi_epi16(x9, x8);
      x1 = _mm256_madd_epi16(x1, c1);
      x3 = _mm256_madd_epi16(x3, c2);
      x5 = _mm256_madd_epi16(x5, c3);
      x7 = _mm256_madd_epi16(x7, c4);
      x2 = _mm256_madd_epi16(x2, c1);
      x4 = _mm256_madd_epi16(x4, c2);
      x6 = _mm256_madd_epi16(x6, c3);
      x8 = _mm256_madd_epi16(x8, c4);
      x1 = _mm256_add_epi32(x1, x3);
      x3 = _mm256_add_epi32(x5, x7);
      x2 = _mm256_add_epi32(x2, x4);
      x4 = _mm256_add_epi32(x6, x8);
      x1 = _mm256_add_epi32(x1, x3);
      x2 = _mm256_add_epi32(x2, x4);
      x1 = _mm256_srai_epi32(x1, 2);
      x2 = _mm256_srai_epi32(x2, 2);
      x1 = _mm256_packs_epi32(x1, x2);
      r5 = _mm256_load_si256((__m256i*)&src2[x]);
      {
        __m256i x3, x4, r7, r8;
        x3 = _mm256_mulhi_epi16(x1, wx1);
        x1 = _mm256_mullo_epi16(x1, wx1);
        r7 = _mm256_mulhi_epi16(r5, wx0);
        r5 = _mm256_mullo_epi16(r5, wx0);
        x4 = _mm256_unpackhi_epi16(x1, x3);
        x1 = _mm256_unpacklo_epi16(x1, x3);
        r8 = _mm256_unpackhi_epi16(r5, r7);
        r5 = _mm256_unpacklo_epi16(r5, r7);
        x4 = _mm256_add_epi32(x4, r8);
        x1 = _mm256_add_epi32(x1, r5);
        x4 = _mm256_srai_epi32(_mm256_add_epi32(x4, offset), shift2);
        x1 = _mm256_srai_epi32(_mm256_add_epi32(x1, offset), shift2);
        x1 = _mm256_packus_epi32(x1, x4);
      };
      x1 = _mm256_max_epi16(x1, _mm256_setzero_si256());
      x1 = _mm256_min_epi16(x1, _mm256_set1_epi16(0x03FF));
      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
    src += srcstride;
    src2 += src2stride;
    dst += dststride;
  }
}

static void
put_vvc_uni_w_qpel_hv16_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int height,
                                       int denom,
                                       int wx,
                                       int ox,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  int16_t tmp_array[(MAX_PB_SIZE + QPEL_EXTRA) * MAX_PB_SIZE];
  int16_t* tmp = tmp_array;
  uint16_t* src = (uint16_t*)_src;
  const int srcstride = _srcstride >> 1;
  src -= QPEL_EXTRA_BEFORE * srcstride;
  put_vvc_qpel_h16_10_avx2(tmp,
                                  MAX_PB_SIZE,
                                  (uint8_t*)src,
                                  _srcstride,
                                  height + QPEL_EXTRA,
                                  mx,
                                  my,
                                  width);
  tmp = tmp_array + QPEL_EXTRA_BEFORE * MAX_PB_SIZE;
  put_vvc_uni_w_qpel_v16_14_10_avx2(dst,
                                           dststride,
                                           (uint8_t*)tmp,
                                           MAX_PB_SIZE << 1,
                                           height,
                                           denom,
                                           wx,
                                           ox,
                                           mx,
                                           my,
                                           width);
}
static void
put_vvc_bi_w_qpel_hv16_10_avx2(uint8_t* dst,
                                      ptrdiff_t dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int16_t* src2,
                                      ptrdiff_t src2stride,
                                      int height,
                                      int denom,
                                      int wx0,
                                      int wx1,


                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  int16_t tmp_array[(MAX_PB_SIZE + QPEL_EXTRA) * MAX_PB_SIZE];
  int16_t* tmp = tmp_array;
  uint16_t* src = (uint16_t*)_src;
  const int srcstride = _srcstride >> 1;
  src -= QPEL_EXTRA_BEFORE * srcstride;
  put_vvc_qpel_h16_10_avx2(tmp,
                                  MAX_PB_SIZE,
                                  (uint8_t*)src,
                                  _srcstride,
                                  height + QPEL_EXTRA,
                                  mx,
                                  my,
                                  width);
  tmp = tmp_array + QPEL_EXTRA_BEFORE * MAX_PB_SIZE;
  put_vvc_bi_w_qpel_v16_14_10_avx2(dst,
                                          dststride,
                                          (uint8_t*)tmp,
                                          MAX_PB_SIZE << 1,
                                          src2,
                                          src2stride,
                                          height,
                                          denom,
                                          wx0,
                                          wx1,


                                          mx,
                                          my,
                                          width);
}
static void
put_vvc_uni_w_pel_pixels16_10_avx2(uint8_t* dst,
                                           ptrdiff_t dststride,
                                           uint8_t* _src,
                                           ptrdiff_t _srcstride,
                                           int height,
                                           int denom,
                                           int wx,
                                           int ox,
                                           intptr_t mx,
                                           intptr_t my,
                                           int width)
{
  put_vvc_uni_w_pel_pixels8_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_pel_pixels16_10_avx2(uint8_t* dst,
                                          ptrdiff_t dststride,
                                          uint8_t* _src,
                                          ptrdiff_t _srcstride,
                                          int16_t* src2,
                                          ptrdiff_t src2stride,
                                          int height,
                                          int denom,
                                          int wx0,
                                          int wx1,


                                          intptr_t mx,
                                          intptr_t my,
                                          int width)
{
  put_vvc_bi_w_pel_pixels8_10_avx2(dst,
                                           dststride,
                                           _src,
                                           _srcstride,
                                           src2,
                                           src2stride,
                                           height,
                                           denom,
                                           wx0,
                                           wx1,


                                           mx,
                                           my,
                                           width);
}

static void
put_vvc_uni_w_pel_pixels32_10_avx2(uint8_t* dst,
                                           ptrdiff_t dststride,
                                           uint8_t* _src,
                                           ptrdiff_t _srcstride,
                                           int height,
                                           int denom,
                                           int wx,
                                           int ox,
                                           intptr_t mx,
                                           intptr_t my,
                                           int width)
{
  put_vvc_uni_w_pel_pixels8_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_pel_pixels32_10_avx2(uint8_t* dst,
                                          ptrdiff_t dststride,
                                          uint8_t* _src,
                                          ptrdiff_t _srcstride,
                                          int16_t* src2,
                                          ptrdiff_t src2stride,
                                          int height,
                                          int denom,
                                          int wx0,
                                          int wx1,
                                          intptr_t mx,
                                          intptr_t my,
                                          int width)
{
  put_vvc_bi_w_pel_pixels8_10_avx2(dst,
                                           dststride,
                                           _src,
                                           _srcstride,
                                           src2,
                                           src2stride,
                                           height,
                                           denom,
                                           wx0,
                                           wx1,
                                           mx,
                                           my,
                                           width);
}

static void
put_vvc_uni_w_pel_pixels64_10_avx2(uint8_t* dst,
                                           ptrdiff_t dststride,
                                           uint8_t* _src,
                                           ptrdiff_t _srcstride,
                                           int height,
                                           int denom,
                                           int wx,
                                           int ox,
                                           intptr_t mx,
                                           intptr_t my,
                                           int width)
{
  put_vvc_uni_w_pel_pixels8_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_pel_pixels64_10_avx2(uint8_t* dst,
                                          ptrdiff_t dststride,
                                          uint8_t* _src,
                                          ptrdiff_t _srcstride,
                                          int16_t* src2,
                                          ptrdiff_t src2stride,
                                          int height,
                                          int denom,
                                          int wx0,
                                          int wx1,


                                          intptr_t mx,
                                          intptr_t my,
                                          int width)
{
  put_vvc_bi_w_pel_pixels8_10_avx2(dst,
                                           dststride,
                                           _src,
                                           _srcstride,
                                           src2,
                                           src2stride,
                                           height,
                                           denom,
                                           wx0,
                                           wx1,


                                           mx,
                                           my,
                                           width);
}

static void
put_vvc_uni_w_qpel_h32_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int height,
                                       int denom,
                                       int wx,
                                       int ox,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  put_vvc_uni_w_qpel_h16_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_qpel_h32_10_avx2(uint8_t* dst,
                                      ptrdiff_t dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int16_t* src2,
                                      ptrdiff_t src2stride,
                                      int height,
                                      int denom,
                                      int wx0,
                                      int wx1,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  put_vvc_bi_w_qpel_h16_10_avx2(dst,
                                       dststride,
                                       _src,
                                       _srcstride,
                                       src2,
                                       src2stride,
                                       height,
                                       denom,
                                       wx0,
                                       wx1,
                                       mx,
                                       my,
                                       width);
}

static void
put_vvc_uni_w_qpel_h64_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int height,
                                       int denom,
                                       int wx,
                                       int ox,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  put_vvc_uni_w_qpel_h16_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_qpel_h64_10_avx2(uint8_t* dst,
                                      ptrdiff_t dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int16_t* src2,
                                      ptrdiff_t src2stride,
                                      int height,
                                      int denom,
                                      int wx0,
                                      int wx1,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  put_vvc_bi_w_qpel_h16_10_avx2(dst,
                                       dststride,
                                       _src,
                                       _srcstride,
                                       src2,
                                       src2stride,
                                       height,
                                       denom,
                                       wx0,
                                       wx1,
                                       mx,
                                       my,
                                       width);
}

static void
put_vvc_uni_w_qpel_v32_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int height,
                                       int denom,
                                       int wx,
                                       int ox,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  put_vvc_uni_w_qpel_v16_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_qpel_v32_10_avx2(uint8_t* dst,
                                      ptrdiff_t dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int16_t* src2,
                                      ptrdiff_t src2stride,
                                      int height,
                                      int denom,
                                      int wx0,
                                      int wx1,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  put_vvc_bi_w_qpel_v16_10_avx2(dst,
                                       dststride,
                                       _src,
                                       _srcstride,
                                       src2,
                                       src2stride,
                                       height,
                                       denom,
                                       wx0,
                                       wx1,
                                       mx,
                                       my,
                                       width);
}

static void
put_vvc_uni_w_qpel_v64_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int height,
                                       int denom,
                                       int wx,
                                       int ox,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  put_vvc_uni_w_qpel_v16_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_qpel_v64_10_avx2(uint8_t* dst,
                                      ptrdiff_t dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int16_t* src2,
                                      ptrdiff_t src2stride,
                                      int height,
                                      int denom,
                                      int wx0,
                                      int wx1,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  put_vvc_bi_w_qpel_v16_10_avx2(dst,
                                       dststride,
                                       _src,
                                       _srcstride,
                                       src2,
                                       src2stride,
                                       height,
                                       denom,
                                       wx0,
                                       wx1,
                                       mx,
                                       my,
                                       width);
}

static void
put_vvc_uni_w_qpel_hv32_10_avx2(uint8_t* dst,
                                        ptrdiff_t dststride,
                                        uint8_t* _src,
                                        ptrdiff_t _srcstride,
                                        int height,
                                        int denom,
                                        int wx,
                                        int ox,
                                        intptr_t mx,
                                        intptr_t my,
                                        int width)
{
  put_vvc_uni_w_qpel_hv16_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_qpel_hv32_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int16_t* src2,
                                       ptrdiff_t src2stride,
                                       int height,
                                       int denom,
                                       int wx0,
                                       int wx1,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  put_vvc_bi_w_qpel_hv16_10_avx2(dst,
                                        dststride,
                                        _src,
                                        _srcstride,
                                        src2,
                                        src2stride,
                                        height,
                                        denom,
                                        wx0,
                                        wx1,
                                        mx,
                                        my,
                                        width);
}

static void
put_vvc_uni_w_qpel_hv64_10_avx2(uint8_t* dst,
                                        ptrdiff_t dststride,
                                        uint8_t* _src,
                                        ptrdiff_t _srcstride,
                                        int height,
                                        int denom,
                                        int wx,
                                        int ox,
                                        intptr_t mx,
                                        intptr_t my,
                                        int width)
{
  put_vvc_uni_w_qpel_hv16_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_qpel_hv64_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int16_t* src2,
                                       ptrdiff_t src2stride,
                                       int height,
                                       int denom,
                                       int wx0,
                                       int wx1,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  put_vvc_bi_w_qpel_hv16_10_avx2(dst,
                                        dststride,
                                        _src,
                                        _srcstride,
                                        src2,
                                        src2stride,
                                        height,
                                        denom,
                                        wx0,
                                        wx1,
                                        mx,
                                        my,
                                        width);
}

static void
put_vvc_uni_w_epel_h32_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int height,
                                       int denom,
                                       int wx,
                                       int ox,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  put_vvc_uni_w_epel_h16_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_epel_h32_10_avx2(uint8_t* dst,
                                      ptrdiff_t dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int16_t* src2,
                                      ptrdiff_t src2stride,
                                      int height,
                                      int denom,
                                      int wx0,
                                      int wx1,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  put_vvc_bi_w_epel_h16_10_avx2(dst,
                                       dststride,
                                       _src,
                                       _srcstride,
                                       src2,
                                       src2stride,
                                       height,
                                       denom,
                                       wx0,
                                       wx1,
                                       mx,
                                       my,
                                       width);
}

static void
put_vvc_uni_w_epel_h64_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int height,
                                       int denom,
                                       int wx,
                                       int ox,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  put_vvc_uni_w_epel_h16_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_epel_h64_10_avx2(uint8_t* dst,
                                      ptrdiff_t dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int16_t* src2,
                                      ptrdiff_t src2stride,
                                      int height,
                                      int denom,
                                      int wx0,
                                      int wx1,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  put_vvc_bi_w_epel_h16_10_avx2(dst,
                                       dststride,
                                       _src,
                                       _srcstride,
                                       src2,
                                       src2stride,
                                       height,
                                       denom,
                                       wx0,
                                       wx1,
                                       mx,
                                       my,
                                       width);
}

static void
put_vvc_uni_w_epel_v32_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int height,
                                       int denom,
                                       int wx,
                                       int ox,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  put_vvc_uni_w_epel_v16_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_epel_v32_10_avx2(uint8_t* dst,
                                      ptrdiff_t dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int16_t* src2,
                                      ptrdiff_t src2stride,
                                      int height,
                                      int denom,
                                      int wx0,
                                      int wx1,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  put_vvc_bi_w_epel_v16_10_avx2(dst,
                                       dststride,
                                       _src,
                                       _srcstride,
                                       src2,
                                       src2stride,
                                       height,
                                       denom,
                                       wx0,
                                       wx1,
                                       mx,
                                       my,
                                       width);
}

static void
put_vvc_uni_w_epel_v64_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int height,
                                       int denom,
                                       int wx,
                                       int ox,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  put_vvc_uni_w_epel_v16_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_epel_v64_10_avx2(uint8_t* dst,
                                      ptrdiff_t dststride,
                                      uint8_t* _src,
                                      ptrdiff_t _srcstride,
                                      int16_t* src2,
                                      ptrdiff_t src2stride,
                                      int height,
                                      int denom,
                                      int wx0,
                                      int wx1,
                                      intptr_t mx,
                                      intptr_t my,
                                      int width)
{
  put_vvc_bi_w_epel_v16_10_avx2(dst,
                                       dststride,
                                       _src,
                                       _srcstride,
                                       src2,
                                       src2stride,
                                       height,
                                       denom,
                                       wx0,
                                       wx1,
                                       mx,
                                       my,
                                       width);
}

static void
put_vvc_uni_w_epel_hv32_10_avx2(uint8_t* dst,
                                        ptrdiff_t dststride,
                                        uint8_t* _src,
                                        ptrdiff_t _srcstride,
                                        int height,
                                        int denom,
                                        int wx,
                                        int ox,
                                        intptr_t mx,
                                        intptr_t my,
                                        int width)
{
  put_vvc_uni_w_epel_hv16_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_epel_hv32_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int16_t* src2,
                                       ptrdiff_t src2stride,
                                       int height,
                                       int denom,
                                       int wx0,
                                       int wx1,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  put_vvc_bi_w_epel_hv16_10_avx2(dst,
                                        dststride,
                                        _src,
                                        _srcstride,
                                        src2,
                                        src2stride,
                                        height,
                                        denom,
                                        wx0,
                                        wx1,
                                        mx,
                                        my,
                                        width);
}

static void
put_vvc_uni_w_epel_hv64_10_avx2(uint8_t* dst,
                                        ptrdiff_t dststride,
                                        uint8_t* _src,
                                        ptrdiff_t _srcstride,
                                        int height,
                                        int denom,
                                        int wx,
                                        int ox,
                                        intptr_t mx,
                                        intptr_t my,
                                        int width)
{
  put_vvc_uni_w_epel_hv16_10_avx2(
    dst, dststride, _src, _srcstride, height, denom, wx, ox, mx, my, width);
}
static void
put_vvc_bi_w_epel_hv64_10_avx2(uint8_t* dst,
                                       ptrdiff_t dststride,
                                       uint8_t* _src,
                                       ptrdiff_t _srcstride,
                                       int16_t* src2,
                                       ptrdiff_t src2stride,
                                       int height,
                                       int denom,
                                       int wx0,
                                       int wx1,
                                       intptr_t mx,
                                       intptr_t my,
                                       int width)
{
  put_vvc_bi_w_epel_hv16_10_avx2(dst,
                                        dststride,
                                        _src,
                                        _srcstride,
                                        src2,
                                        src2stride,
                                        height,
                                        denom,
                                        wx0,
                                        wx1,
                                        mx,
                                        my,
                                        width);
}
#endif

static void
put_vvc_qpel_bilinear_h_avx2(uint16_t* _dst, ptrdiff_t _dststride, const uint16_t* _src,
                        ptrdiff_t _srcstride, int height, intptr_t mx, intptr_t my,
                        int width)
{
    int x, y;
    __m256i x1, x2, t1, t2;
    const uint16_t* src = ((uint16_t*)_src);
    ptrdiff_t srcstride = _srcstride;
    uint16_t* dst = (uint16_t*)_dst;
    ptrdiff_t dststride = _dststride;
    const int16_t* filter = ov_bilinear_filters_4[mx - 1];
    int shift = 14 - BIT_DEPTH;
    __m256i c = _mm256_loadu_si256((__m256i*)filter);
    __m256i offset = _mm256_set1_epi32(1 << (shift - 1));

    for (y = 0; y < height; y++) {
      for (x = 0; x < width; x+=16) {
        x1 = _mm256_loadu_si256((__m256i*)&src[x]);
        x2 = _mm256_loadu_si256((__m256i*)&src[x + 1]);

        t1 = _mm256_unpacklo_epi16(x1, x2);
        t2 = _mm256_unpackhi_epi16(x1, x2);

        t1 = _mm256_madd_epi16(t1, c);
        t2 = _mm256_madd_epi16(t2, c);

        x1 = _mm256_add_epi32(t1, offset);
        x2 = _mm256_add_epi32(t2, offset);

        x1 = _mm256_srai_epi32(x1, shift);
        x2 = _mm256_srai_epi32(x2, shift);

        x1 = _mm256_max_epi32(x1, _mm256_setzero_si256());
        x2 = _mm256_max_epi32(x2, _mm256_setzero_si256());

        x1 = _mm256_min_epi32(x1, _mm256_set1_epi32(1023));
        x2 = _mm256_min_epi32(x2, _mm256_set1_epi32(1023));

        x1 = _mm256_packs_epi32(x1,x2);

        _mm256_storeu_si256((__m256i*)&dst[x], x1);
        }
        src += srcstride;
        dst += dststride;
    }
}

static void
put_vvc_qpel_bilinear_v_avx2(uint16_t* _dst, ptrdiff_t _dststride, const uint16_t* _src,
                        ptrdiff_t _srcstride, int height, intptr_t mx, intptr_t my,
                        int width)
{
  {
      int x, y;
      __m256i x1, x2, t1, t2;
      const uint16_t* src = ((uint16_t*)_src);
      ptrdiff_t srcstride = _srcstride;
      uint16_t* dst = (uint16_t*)_dst;
      ptrdiff_t dststride = _dststride;
      const int16_t* filter = ov_bilinear_filters_4[my - 1];
      int shift = 14 - BIT_DEPTH;
      __m256i c = _mm256_loadu_si256((__m256i*)filter);
      __m256i offset = _mm256_set1_epi32(1 << (shift - 1));

      for (y = 0; y < height; y++) {
        for (x = 0; x < width; x+=16) {
          x1 = _mm256_loadu_si256((__m256i*)&src[x]);
          x2 = _mm256_loadu_si256((__m256i*)&src[x + srcstride]);

          t1 = _mm256_unpacklo_epi16(x1, x2);
          t2 = _mm256_unpackhi_epi16(x1, x2);

          t1 = _mm256_madd_epi16(t1, c);
          t2 = _mm256_madd_epi16(t2, c);

          x1 = _mm256_add_epi32(t1, offset);
          x2 = _mm256_add_epi32(t2, offset);

          x1 = _mm256_srai_epi32(x1, shift);
          x2 = _mm256_srai_epi32(x2, shift);

          x1 = _mm256_max_epi32(x1, _mm256_setzero_si256());
          x2 = _mm256_max_epi32(x2, _mm256_setzero_si256());

          x1 = _mm256_min_epi32(x1, _mm256_set1_epi32(1023));
          x2 = _mm256_min_epi32(x2, _mm256_set1_epi32(1023));

          x1 = _mm256_packs_epi32(x1,x2);

          _mm256_storeu_si256((__m256i*)&dst[x], x1);
          }
          src += srcstride;
          dst += dststride;
      }
  }
}

static void
put_vvc_qpel_bilinear_hv_avx2(uint16_t* _dst, ptrdiff_t _dststride, const uint16_t* _src,
                         ptrdiff_t _srcstride, int height, intptr_t mx, intptr_t my,
                         int width)
{
  uint16_t tmp_array[(MAX_PB_SIZE + QPEL_EXTRA) * MAX_PB_SIZE];
  uint16_t* tmp = tmp_array;
  put_vvc_qpel_bilinear_h_avx2(tmp, MAX_PB_SIZE, _src-1, _srcstride, height+2, mx, my, width+2);
  tmp = tmp_array + 1;
  put_vvc_qpel_bilinear_v_avx2(_dst, _dststride, tmp, MAX_PB_SIZE, height, mx, my, width);
}

static void
put_weighted_ciip_pixels_avx2(uint16_t* dst, int dststride,
                      const uint16_t* src_intra, const uint16_t* src_inter, int srcstride,
                      int width, int height, int wt)
{
  int x, y;
  int shift  = 2;
  __m256i x1, x2, t1, t2;
  __m256i c1 = _mm256_set1_epi16(wt);
  __m256i c2 = _mm256_set1_epi16(4-wt);
  __m256i c = _mm256_unpacklo_epi16(c1, c2);
  __m256i offset = _mm256_set1_epi32(1 << (shift - 1));

  __m256i x1l, x2l, t1l, t2l;
  __m256i c1l = _mm256_set1_epi16(wt);
  __m256i c2l = _mm256_set1_epi16(4-wt);
  __m256i cl = _mm256_unpacklo_epi16(c1l, c2l);
  __m256i offsetl = _mm256_set1_epi32(1 << (shift - 1));
  for (y = 0; y < height; y++) {
    for (x = 0; x < width - width%16; x+=16) {
      x1l = _mm256_loadu_si256((__m256i*)&src_intra[x]);
      x2l = _mm256_loadu_si256((__m256i*)&src_inter[x]);

      t1l = _mm256_unpacklo_epi16(x1l, x2l);
      t2l = _mm256_unpackhi_epi16(x1l, x2l);

      t1l = _mm256_madd_epi16(t1l, cl);
      t2l = _mm256_madd_epi16(t2l, cl);

      x1l = _mm256_add_epi32(t1l, offsetl);
      x2l = _mm256_add_epi32(t2l, offsetl);

      x1l = _mm256_srai_epi32(x1l, shift);
      x2l = _mm256_srai_epi32(x2l, shift);

      x1l = _mm256_max_epi32(x1l, _mm256_setzero_si256());
      x2l = _mm256_max_epi32(x2l, _mm256_setzero_si256());

      x1l = _mm256_min_epi32(x1l, _mm256_set1_epi32(1023));
      x2l = _mm256_min_epi32(x2l, _mm256_set1_epi32(1023));

      x1l = _mm256_packs_epi32(x1l,x2l);

      _mm256_storeu_si256((__m256i*)&dst[x], x1l);
    }
    for (; x < width; x+=16) {
      x1 = _mm256_loadu_si256((__m256i*)&src_intra[x]);
      x2 = _mm256_loadu_si256((__m256i*)&src_inter[x]);

      t1 = _mm256_unpacklo_epi16(x1, x2);
      t2 = _mm256_unpackhi_epi16(x1, x2);

      t1 = _mm256_madd_epi16(t1, c);
      t2 = _mm256_madd_epi16(t2, c);

      x1 = _mm256_add_epi32(t1, offset);
      x2 = _mm256_add_epi32(t2, offset);

      x1 = _mm256_srai_epi32(x1, shift);
      x2 = _mm256_srai_epi32(x2, shift);

      x1 = _mm256_max_epi32(x1, _mm256_setzero_si256());
      x2 = _mm256_max_epi32(x2, _mm256_setzero_si256());

      x1 = _mm256_min_epi32(x1, _mm256_set1_epi32(1023));
      x2 = _mm256_min_epi32(x2, _mm256_set1_epi32(1023));

      x1 = _mm256_packs_epi32(x1,x2);

      _mm256_storeu_si256((__m256i*)&dst[x], x1);
    }
      src_intra += srcstride;
      src_inter += srcstride;
      dst += dststride;
  }
}

void
rcn_init_mc_functions_avx2(struct RCNFunctions* const rcn_funcs)
{
  struct MCFunctions* const mc_l = &rcn_funcs->mc_l;
  struct MCFunctions* const mc_c = &rcn_funcs->mc_c;

  /* Luma functions */
  mc_l->bidir0[0][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi0_pel_pixels16_10_avx2;
  mc_l->bidir1[0][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi1_pel_pixels16_10_avx2;
  #if WEIGHTED
  mc_l->unidir_w[0][SIZE_BLOCK_16] = &put_vvc_uni_w_pel_pixels16_10_avx2;
  mc_l->bidir_w[0][SIZE_BLOCK_16] = &put_vvc_bi_w_pel_pixels16_10_avx2;
  #endif

  mc_l->unidir[1][SIZE_BLOCK_16] = &oh_hevc_put_hevc_uni_qpel_h16_10_avx2;
  mc_l->bidir0[1][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi0_qpel_h16_10_avx2;
  mc_l->bidir1[1][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi1_qpel_h16_10_avx2;
  mc_l->bilinear[1][SIZE_BLOCK_16] = &put_vvc_qpel_bilinear_h_avx2;
  #if WEIGHTED
  mc_l->unidir_w[1][SIZE_BLOCK_16] = &put_vvc_uni_w_qpel_h16_10_avx2;
  mc_l->bidir_w[1][SIZE_BLOCK_16] = &put_vvc_bi_w_qpel_h16_10_avx2;
  #endif

  mc_l->unidir[2][SIZE_BLOCK_16] = &oh_hevc_put_hevc_uni_qpel_v16_10_avx2;
  mc_l->bidir0[2][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi0_qpel_v16_10_avx2;
  mc_l->bidir1[2][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi1_qpel_v16_10_avx2;
  mc_l->bilinear[2][SIZE_BLOCK_16] = &put_vvc_qpel_bilinear_v_avx2;
  #if WEIGHTED
  mc_l->unidir_w[2][SIZE_BLOCK_16] = &put_vvc_uni_w_qpel_v16_10_avx2;
  mc_l->bidir_w[2][SIZE_BLOCK_16] = &put_vvc_bi_w_qpel_v16_10_avx2;
  #endif

  mc_l->unidir[3][SIZE_BLOCK_16] = &oh_hevc_put_hevc_uni_qpel_hv16_10_avx2;
  mc_l->bidir0[3][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi0_qpel_hv16_10_avx2;
  mc_l->bidir1[3][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi1_qpel_hv16_10_avx2;
  mc_l->bilinear[3][SIZE_BLOCK_16] = &put_vvc_qpel_bilinear_hv_avx2;
  #if WEIGHTED
  mc_l->unidir_w[3][SIZE_BLOCK_16] = &put_vvc_uni_w_qpel_hv16_10_avx2;
  mc_l->bidir_w[3][SIZE_BLOCK_16] = &put_vvc_bi_w_qpel_hv16_10_avx2;
  #endif

  mc_l->bidir0[0][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi0_pel_pixels32_10_avx2;
  mc_l->bidir1[0][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi1_pel_pixels32_10_avx2;
  #if WEIGHTED
  mc_l->unidir_w[0][SIZE_BLOCK_32] = &put_vvc_uni_w_pel_pixels32_10_avx2;
  mc_l->bidir_w[0][SIZE_BLOCK_32] = &put_vvc_bi_w_pel_pixels32_10_avx2;
  #endif

  mc_l->unidir[1][SIZE_BLOCK_32] = &oh_hevc_put_hevc_uni_qpel_h32_10_avx2;
  mc_l->bidir0[1][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi0_qpel_h32_10_avx2;
  mc_l->bidir1[1][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi1_qpel_h32_10_avx2;
  mc_l->bilinear[1][SIZE_BLOCK_32] = &put_vvc_qpel_bilinear_h_avx2;
  #if WEIGHTED
  mc_l->unidir_w[1][SIZE_BLOCK_32] = &put_vvc_uni_w_qpel_h32_10_avx2;
  mc_l->bidir_w[1][SIZE_BLOCK_32] = &put_vvc_bi_w_qpel_h32_10_avx2;
  #endif

  mc_l->unidir[2][SIZE_BLOCK_32] = &oh_hevc_put_hevc_uni_qpel_v32_10_avx2;
  mc_l->bidir0[2][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi0_qpel_v32_10_avx2;
  mc_l->bidir1[2][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi1_qpel_v32_10_avx2;
  mc_l->bilinear[2][SIZE_BLOCK_32] = &put_vvc_qpel_bilinear_v_avx2;
  #if WEIGHTED
  mc_l->unidir_w[2][SIZE_BLOCK_32] = &put_vvc_uni_w_qpel_v32_10_avx2;
  mc_l->bidir_w[2][SIZE_BLOCK_32] = &put_vvc_bi_w_qpel_v32_10_avx2;
  #endif

  mc_l->unidir[3][SIZE_BLOCK_32] = &oh_hevc_put_hevc_uni_qpel_hv32_10_avx2;
  mc_l->bidir0[3][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi0_qpel_hv32_10_avx2;
  mc_l->bidir1[3][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi1_qpel_hv32_10_avx2;
  mc_l->bilinear[3][SIZE_BLOCK_32] = &put_vvc_qpel_bilinear_hv_avx2;
  #if WEIGHTED
  mc_l->unidir_w[3][SIZE_BLOCK_32] = &put_vvc_uni_w_qpel_hv32_10_avx2;
  mc_l->bidir_w[3][SIZE_BLOCK_32] = &put_vvc_bi_w_qpel_hv32_10_avx2;
  #endif

  mc_l->bidir0[0][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi0_pel_pixels64_10_avx2;
  mc_l->bidir1[0][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi1_pel_pixels64_10_avx2;
  #if WEIGHTED
  mc_l->unidir_w[0][SIZE_BLOCK_64] = &put_vvc_uni_w_pel_pixels64_10_avx2;
  mc_l->bidir_w[0][SIZE_BLOCK_64] = &put_vvc_bi_w_pel_pixels64_10_avx2;
  #endif

  mc_l->unidir[1][SIZE_BLOCK_64] = &oh_hevc_put_hevc_uni_qpel_h64_10_avx2;
  mc_l->bidir0[1][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi0_qpel_h64_10_avx2;
  mc_l->bidir1[1][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi1_qpel_h64_10_avx2;
  mc_l->bilinear[1][SIZE_BLOCK_64] = &put_vvc_qpel_bilinear_h_avx2;
  #if WEIGHTED
  mc_l->unidir_w[1][SIZE_BLOCK_64] = &put_vvc_uni_w_qpel_h64_10_avx2;
  mc_l->bidir_w[1][SIZE_BLOCK_64] = &put_vvc_bi_w_qpel_h64_10_avx2;
  #endif

  mc_l->unidir[2][SIZE_BLOCK_64] = &oh_hevc_put_hevc_uni_qpel_v64_10_avx2;
  mc_l->bidir0[2][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi0_qpel_v64_10_avx2;
  mc_l->bidir1[2][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi1_qpel_v64_10_avx2;
  mc_l->bilinear[2][SIZE_BLOCK_64] = &put_vvc_qpel_bilinear_v_avx2;
  #if WEIGHTED
  mc_l->unidir_w[2][SIZE_BLOCK_64] = &put_vvc_uni_w_qpel_v64_10_avx2;
  mc_l->bidir_w[2][SIZE_BLOCK_64] = &put_vvc_bi_w_qpel_v64_10_avx2;
  #endif

  mc_l->unidir[3][SIZE_BLOCK_64] = &oh_hevc_put_hevc_uni_qpel_hv64_10_avx2;
  mc_l->bidir0[3][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi0_qpel_hv64_10_avx2;
  mc_l->bidir1[3][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi1_qpel_hv64_10_avx2;
  mc_l->bilinear[3][SIZE_BLOCK_64] = &put_vvc_qpel_bilinear_hv_avx2;
  #if WEIGHTED
  mc_l->unidir_w[3][SIZE_BLOCK_64] = &put_vvc_uni_w_qpel_hv64_10_avx2;
  mc_l->bidir_w[3][SIZE_BLOCK_64] = &put_vvc_bi_w_qpel_hv64_10_avx2;
  #endif

  mc_l->bidir0[0][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi0_pel_pixels64_10_avx2;
  mc_l->bidir1[0][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi1_pel_pixels64_10_avx2;
  #if WEIGHTED
  mc_l->unidir_w[0][SIZE_BLOCK_128] = &put_vvc_uni_w_pel_pixels64_10_avx2;
  mc_l->bidir_w[0][SIZE_BLOCK_128] = &put_vvc_bi_w_pel_pixels64_10_avx2;
  #endif

  mc_l->unidir[1][SIZE_BLOCK_128] = &oh_hevc_put_hevc_uni_qpel_h64_10_avx2;
  mc_l->bidir0[1][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi0_qpel_h64_10_avx2;
  mc_l->bidir1[1][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi1_qpel_h64_10_avx2;
  mc_l->bilinear[1][SIZE_BLOCK_128] = &put_vvc_qpel_bilinear_h_avx2;
  #if WEIGHTED
  mc_l->unidir_w[1][SIZE_BLOCK_128] = &put_vvc_uni_w_qpel_h64_10_avx2;
  mc_l->bidir_w[1][SIZE_BLOCK_128] = &put_vvc_bi_w_qpel_h64_10_avx2;
  #endif

  mc_l->unidir[2][SIZE_BLOCK_128] = &oh_hevc_put_hevc_uni_qpel_v64_10_avx2;
  mc_l->bidir0[2][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi0_qpel_v64_10_avx2;
  mc_l->bidir1[2][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi1_qpel_v64_10_avx2;
  mc_l->bilinear[2][SIZE_BLOCK_128] = &put_vvc_qpel_bilinear_v_avx2;
  #if WEIGHTED
  mc_l->unidir_w[2][SIZE_BLOCK_128] = &put_vvc_uni_w_qpel_v64_10_avx2;
  mc_l->bidir_w[2][SIZE_BLOCK_128] = &put_vvc_bi_w_qpel_v64_10_avx2;
  #endif

  mc_l->unidir[3][SIZE_BLOCK_128] = &oh_hevc_put_hevc_uni_qpel_hv64_10_avx2;
  mc_l->bidir0[3][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi0_qpel_hv64_10_avx2;
  mc_l->bidir1[3][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi1_qpel_hv64_10_avx2;
  mc_l->bilinear[3][SIZE_BLOCK_128] = &put_vvc_qpel_bilinear_hv_avx2;
  #if WEIGHTED
  mc_l->unidir_w[3][SIZE_BLOCK_128] = &put_vvc_uni_w_qpel_hv64_10_avx2;
  mc_l->bidir_w[3][SIZE_BLOCK_128] = &put_vvc_bi_w_qpel_hv64_10_avx2;
  #endif


  /* Chroma functions */
  mc_c->bidir0[0][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi0_pel_pixels16_10_avx2;
  mc_c->bidir1[0][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi1_pel_pixels16_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[0][SIZE_BLOCK_16] = &put_vvc_uni_w_pel_pixels16_10_avx2;
  mc_c->bidir_w[0][SIZE_BLOCK_16] = &put_vvc_bi_w_pel_pixels16_10_avx2;
  #endif

  mc_c->unidir[1][SIZE_BLOCK_16] = &oh_hevc_put_hevc_uni_epel_h16_10_avx2;
  mc_c->bidir0[1][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi0_epel_h16_10_avx2;
  mc_c->bidir1[1][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi1_epel_h16_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[1][SIZE_BLOCK_16] = &put_vvc_uni_w_epel_h16_10_avx2;
  mc_c->bidir_w[1][SIZE_BLOCK_16] = &put_vvc_bi_w_epel_h16_10_avx2;
  #endif

  mc_c->unidir[2][SIZE_BLOCK_16] = &oh_hevc_put_hevc_uni_epel_v16_10_avx2;
  mc_c->bidir0[2][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi0_epel_v16_10_avx2;
  mc_c->bidir1[2][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi1_epel_v16_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[2][SIZE_BLOCK_16] = &put_vvc_uni_w_epel_v16_10_avx2;
  mc_c->bidir_w[2][SIZE_BLOCK_16] = &put_vvc_bi_w_epel_v16_10_avx2;
  #endif

  mc_c->unidir[3][SIZE_BLOCK_16] = &oh_hevc_put_hevc_uni_epel_hv16_10_avx2;
  mc_c->bidir0[3][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi0_epel_hv16_10_avx2;
  mc_c->bidir1[3][SIZE_BLOCK_16] = &oh_hevc_put_hevc_bi1_epel_hv16_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[3][SIZE_BLOCK_16] = &put_vvc_uni_w_epel_hv16_10_avx2;
  mc_c->bidir_w[3][SIZE_BLOCK_16] = &put_vvc_bi_w_epel_hv16_10_avx2;
  #endif

  mc_c->bidir0[0][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi0_pel_pixels32_10_avx2;
  mc_c->bidir1[0][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi1_pel_pixels32_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[0][SIZE_BLOCK_32] = &put_vvc_uni_w_pel_pixels32_10_avx2;
  mc_c->bidir_w[0][SIZE_BLOCK_32] = &put_vvc_bi_w_pel_pixels32_10_avx2;
  #endif

  mc_c->unidir[1][SIZE_BLOCK_32] = &oh_hevc_put_hevc_uni_epel_h32_10_avx2;
  mc_c->bidir0[1][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi0_epel_h32_10_avx2;
  mc_c->bidir1[1][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi1_epel_h32_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[1][SIZE_BLOCK_32] = &put_vvc_uni_w_epel_h32_10_avx2;
  mc_c->bidir_w[1][SIZE_BLOCK_32] = &put_vvc_bi_w_epel_h32_10_avx2;
  #endif

  mc_c->unidir[2][SIZE_BLOCK_32] = &oh_hevc_put_hevc_uni_epel_v32_10_avx2;
  mc_c->bidir0[2][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi0_epel_v32_10_avx2;
  mc_c->bidir1[2][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi1_epel_v32_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[2][SIZE_BLOCK_32] = &put_vvc_uni_w_epel_v32_10_avx2;
  mc_c->bidir_w[2][SIZE_BLOCK_32] = &put_vvc_bi_w_epel_v32_10_avx2;
  #endif

  mc_c->unidir[3][SIZE_BLOCK_32] = &oh_hevc_put_hevc_uni_epel_hv32_10_avx2;
  mc_c->bidir0[3][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi0_epel_hv32_10_avx2;
  mc_c->bidir1[3][SIZE_BLOCK_32] = &oh_hevc_put_hevc_bi1_epel_hv32_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[3][SIZE_BLOCK_32] = &put_vvc_uni_w_epel_hv32_10_avx2;
  mc_c->bidir_w[3][SIZE_BLOCK_32] = &put_vvc_bi_w_epel_hv32_10_avx2;
  #endif

  mc_c->bidir0[0][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi0_pel_pixels64_10_avx2;
  mc_c->bidir1[0][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi1_pel_pixels64_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[0][SIZE_BLOCK_64] = &put_vvc_uni_w_pel_pixels64_10_avx2;
  mc_c->bidir_w[0][SIZE_BLOCK_64] = &put_vvc_bi_w_pel_pixels64_10_avx2;
  #endif

  mc_c->unidir[1][SIZE_BLOCK_64] = &oh_hevc_put_hevc_uni_epel_h64_10_avx2;
  mc_c->bidir0[1][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi0_epel_h64_10_avx2;
  mc_c->bidir1[1][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi1_epel_h64_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[1][SIZE_BLOCK_64] = &put_vvc_uni_w_epel_h64_10_avx2;
  mc_c->bidir_w[1][SIZE_BLOCK_64] = &put_vvc_bi_w_epel_h64_10_avx2;
  #endif

  mc_c->unidir[2][SIZE_BLOCK_64] = &oh_hevc_put_hevc_uni_epel_v64_10_avx2;
  mc_c->bidir0[2][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi0_epel_v64_10_avx2;
  mc_c->bidir1[2][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi1_epel_v64_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[2][SIZE_BLOCK_64] = &put_vvc_uni_w_epel_v64_10_avx2;
  mc_c->bidir_w[2][SIZE_BLOCK_64] = &put_vvc_bi_w_epel_v64_10_avx2;
  #endif

  mc_c->unidir[3][SIZE_BLOCK_64] = &oh_hevc_put_hevc_uni_epel_hv64_10_avx2;
  mc_c->bidir0[3][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi0_epel_hv64_10_avx2;
  mc_c->bidir1[3][SIZE_BLOCK_64] = &oh_hevc_put_hevc_bi1_epel_hv64_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[3][SIZE_BLOCK_64] = &put_vvc_uni_w_epel_hv64_10_avx2;
  mc_c->bidir_w[3][SIZE_BLOCK_64] = &put_vvc_bi_w_epel_hv64_10_avx2;
  #endif

  mc_c->bidir0[0][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi0_pel_pixels64_10_avx2;
  mc_c->bidir1[0][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi1_pel_pixels64_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[0][SIZE_BLOCK_128] = &put_vvc_uni_w_pel_pixels64_10_avx2;
  mc_c->bidir_w[0][SIZE_BLOCK_128] = &put_vvc_bi_w_pel_pixels64_10_avx2;
  #endif

  mc_c->unidir[1][SIZE_BLOCK_128] = &oh_hevc_put_hevc_uni_epel_h64_10_avx2;
  mc_c->bidir0[1][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi0_epel_h64_10_avx2;
  mc_c->bidir1[1][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi1_epel_h64_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[1][SIZE_BLOCK_128] = &put_vvc_uni_w_epel_h64_10_avx2;
  mc_c->bidir_w[1][SIZE_BLOCK_128] = &put_vvc_bi_w_epel_h64_10_avx2;
  #endif

  mc_c->unidir[2][SIZE_BLOCK_128] = &oh_hevc_put_hevc_uni_epel_v64_10_avx2;
  mc_c->bidir0[2][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi0_epel_v64_10_avx2;
  mc_c->bidir1[2][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi1_epel_v64_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[2][SIZE_BLOCK_128] = &put_vvc_uni_w_epel_v64_10_avx2;
  mc_c->bidir_w[2][SIZE_BLOCK_128] = &put_vvc_bi_w_epel_v64_10_avx2;
  #endif

  mc_c->unidir[3][SIZE_BLOCK_128] = &oh_hevc_put_hevc_uni_epel_hv64_10_avx2;
  mc_c->bidir0[3][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi0_epel_hv64_10_avx2;
  mc_c->bidir1[3][SIZE_BLOCK_128] = &oh_hevc_put_hevc_bi1_epel_hv64_10_avx2;
  #if WEIGHTED
  mc_c->unidir_w[3][SIZE_BLOCK_128] = &put_vvc_uni_w_epel_hv64_10_avx2;
  mc_c->bidir_w[3][SIZE_BLOCK_128] = &put_vvc_bi_w_epel_hv64_10_avx2;
  #endif
}

void rcn_init_ciip_functions_avx2(struct RCNFunctions *const rcn_funcs)
{
    struct CIIPFunctions *const ciip = &rcn_funcs->ciip;
    ciip->weighted = &put_weighted_ciip_pixels_avx2;
}
